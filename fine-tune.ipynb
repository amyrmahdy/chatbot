{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38abe9b9",
   "metadata": {},
   "source": [
    "### csv_to_jsonl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2110193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import csv\n",
    "# import json\n",
    "\n",
    "# input_csv = \"qa.csv\"\n",
    "# output_jsonl = \"qa.jsonl\"\n",
    "\n",
    "# with open(input_csv, mode='r', encoding='utf-8') as csv_file, open(output_jsonl, mode='w', encoding='utf-8') as jsonl_file:\n",
    "#     reader = csv.DictReader(csv_file)\n",
    "#     for row in reader:\n",
    "#         json.dump(row, jsonl_file, ensure_ascii=False)\n",
    "#         jsonl_file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9211a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORA_ADAPTER_DIR = \"./gemma-qa-lora-final\"\n",
    "# MERGED_MODEL_DIR = \"./gemma-qa-merged-4bit\"\n",
    "\n",
    "\n",
    "# def format_example(example):\n",
    "#     \"\"\"Convert each sample to the instruction-tuning prompt format.\"\"\"\n",
    "#     return {\n",
    "#         \"text\": (\n",
    "#             \"### Ø¯Ø³ØªÙˆØ± Ø§Ù„Ø¹Ù…Ù„:\\n\"\n",
    "#             f\"{example['instruction']}\\n\\n\"\n",
    "#             \"### ÙˆØ±ÙˆØ¯ÛŒ:\\n\"\n",
    "#             f\"{example['input']}\\n\\n\"\n",
    "#             \"### Ø®Ø±ÙˆØ¬ÛŒ:\\n\"\n",
    "#             f\"{example['output']}\"\n",
    "#         )\n",
    "#     }\n",
    "\n",
    "\n",
    "# def answer_the_question(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Returns an answer of the supplied question.\n",
    "#     Deterministic greedy decoding is applied.\n",
    "#     \"\"\"\n",
    "#     prompt = (\n",
    "#         \"### Ø¯Ø³ØªÙˆØ± Ø§Ù„Ø¹Ù…Ù„:\\nØ¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡ Ø¨Ù‡ Ø³ÙˆØ§Ù„\\n\\n\"\n",
    "#         f\"### ÙˆØ±ÙˆØ¯ÛŒ:\\n{text}\\n\\n\"\n",
    "#         \"### Ù¾Ø§Ø³Ø®:\\n\"\n",
    "#     )\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens = MAX_NEW_TOKENS,\n",
    "#             do_sample      = False,                 # greedy decoding\n",
    "#             eos_token_id   = tokenizer.eos_token_id,\n",
    "#         )\n",
    "\n",
    "#     decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     # Extract text following the final '### Response:' tag\n",
    "#     return decoded.split(\"### Ù¾Ø§Ø³Ø®:\")[-1].strip()\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nQuestion â†’ Answer  (Gemma-3.4B, merged).\")\n",
    "# print(\"Enter a question. Type 'q' or 'quit' to exit.\\n\")\n",
    "\n",
    "# while True:\n",
    "#     question = input(\"Ø³ÙˆØ§Ù„>\").strip()\n",
    "#     if question.lower() in {\"q\", \"quit\"}:\n",
    "#         print(\"Session terminated.\")\n",
    "#         break\n",
    "#     if not question:\n",
    "#         continue\n",
    "#     answer = answer_the_question(question)\n",
    "#     print(f\"Ø¬ÙˆØ§Ø¨> {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c631b",
   "metadata": {},
   "source": [
    "### fine_tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f072835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ================================\n",
    "# ðŸ”§ Configuration\n",
    "# ================================\n",
    "\n",
    "# Base pretrained model (QLoRA-compatible) to be fine-tuned\n",
    "BASE_MODEL_PATH = \"./models/gemma-3-4b-it\"\n",
    "\n",
    "# JSONL dataset containing fields: instruction, input, output\n",
    "DATASET_PATH = \"french_translations_1000.jsonl\"\n",
    "\n",
    "# Directory for the final, cleaned LoRA adapter\n",
    "FINAL_MODEL_DIR = \"./gemma-french-lora-final\"\n",
    "\n",
    "# Directory for temporary Trainer checkpoints\n",
    "TEMP_CHECKPOINT_DIR = \"./gemma-french-lora-tmp\"\n",
    "\n",
    "# Maximum sequence length accepted by the model\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "# Micro-batch size per GPU\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Gradient accumulation to simulate a larger batch size\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# Learning rate for LoRA updates\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Number of complete passes through the dataset\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Frequency (in steps) of checkpoint creation\n",
    "SAVE_STEPS = 300\n",
    "\n",
    "# Precision flags\n",
    "USE_FP16 = False   # Enable if the model is loaded in float16\n",
    "USE_BF16 = True    # Enable if the model is loaded in bfloat16\n",
    "\n",
    "# Transformer sub-modules to be adapted by LoRA\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projections\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"      # Feed-forward network\n",
    "]\n",
    "\n",
    "# Disable Weights & Biases logging (optional)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# ================================\n",
    "# ðŸ”¢ Step 1 â€“ Load the quantised base model\n",
    "# ================================\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_PATH,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# ðŸ§  Step 2 â€“ Attach LoRA adapters\n",
    "# ================================\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,                       # LoRA rank\n",
    "    lora_alpha=16,             # Scaling factor\n",
    "    lora_dropout=0.05,         # Regularisation\n",
    "    bias=\"none\",               # Bias parameters kept frozen\n",
    "    target_modules=TARGET_MODULES,\n",
    "    use_gradient_checkpointing=True,  # Memory optimisation\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# ðŸ“š Step 3 â€“ Load and format the dataset\n",
    "# ================================\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "\n",
    "def format_example(example):\n",
    "    \"\"\"Convert each sample to the instruction-tuning prompt format.\"\"\"\n",
    "    return {\n",
    "        \"text\": (\n",
    "            \"### Instruction:\\n\"\n",
    "            f\"{example['instruction']}\\n\\n\"\n",
    "            \"### Input:\\n\"\n",
    "            f\"{example['input']}\\n\\n\"\n",
    "            \"### Response:\\n\"\n",
    "            f\"{example['output']}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_example)\n",
    "\n",
    "# ================================\n",
    "# ðŸ‹ï¸ Step 4 â€“ Fine-tune with SFTTrainer\n",
    "# ================================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=TEMP_CHECKPOINT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=1,\n",
    "        fp16=USE_FP16,\n",
    "        bf16=USE_BF16,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ================================\n",
    "# ðŸ’¾ Step 5 â€“ Save the final LoRA adapter\n",
    "# ================================\n",
    "model.save_pretrained(FINAL_MODEL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "print(f\"\\nâœ… Fine-tuning completed. Adapter saved to: {FINAL_MODEL_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755de3d",
   "metadata": {},
   "source": [
    "### merge_and_save.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_and_save_4bit.py  â€“ merge LoRA â†’ keep 4-bit\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_PATH  = \"./models/gemma-3-4b-it\"\n",
    "LORA_ADAPTER_DIR = \"./gemma-french-lora-final\"\n",
    "MERGED_MODEL_DIR = \"./gemma-french-merged-4bit\"\n",
    "MAX_SEQ_LENGTH   = 512\n",
    "DTYPE            = torch.bfloat16                 # still used for activations\n",
    "\n",
    "# 1. Load base model in 4-bit\n",
    "base_model, tok = FastLanguageModel.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,          # affects forward pass, not weight dtype\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# 2. Wrap with PEFT and attach LoRA\n",
    "peft_model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_DIR)\n",
    "\n",
    "# 3. Merge LoRA into 4-bit backbone\n",
    "print(\"ðŸ”„  Merging (4-bit) â€¦\")\n",
    "merged = peft_model.merge_and_unload()          # stays 4-bit\n",
    "# 4. **Do NOT cast dtype** â€“ keep quantised weights\n",
    "merged.save_pretrained(MERGED_MODEL_DIR, safe_serialization=True)\n",
    "tok.save_pretrained(MERGED_MODEL_DIR)\n",
    "print(\"âœ…  4-bit merged model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe47928",
   "metadata": {},
   "source": [
    "### inference_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "inference_translate.py\n",
    "--------------------------------------------------\n",
    "Loads the merged Gemma-3.4B checkpoint (LoRA already integrated) and\n",
    "offers an interactive terminal loop:\n",
    "\n",
    "    â€¢ An English sentence is entered.\n",
    "    â€¢ A deterministic French translation is returned.\n",
    "    â€¢ Typing â€œqâ€ or â€œquitâ€ terminates the session.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ---------------------------- configuration ----------------------------\n",
    "MERGED_MODEL_DIR  = \"./gemma-french-merged-4bit\"  # produced by merge_and_save.py\n",
    "MAX_SEQ_LENGTH    = 512\n",
    "MAX_NEW_TOKENS    = 60\n",
    "DEVICE            = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE             = torch.bfloat16            # precision used when saving\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Load the standalone merged checkpoint (no LoRA adapter required)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = MERGED_MODEL_DIR,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype          = DTYPE,\n",
    "    load_in_4bit   = False,      # full-precision merged weights\n",
    ")\n",
    "\n",
    "\n",
    "def translate_en_to_fr(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a French translation of the supplied English sentence.\n",
    "    Deterministic greedy decoding is applied.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"### Instruction:\\nTranslate to French\\n\\n\"\n",
    "        f\"### Input:\\n{text}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens = MAX_NEW_TOKENS,\n",
    "            do_sample      = False,                 # greedy decoding\n",
    "            eos_token_id   = tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract text following the final '### Response:' tag\n",
    "    return decoded.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# --------------------------- interactive loop --------------------------\n",
    "print(\"\\nEnglish â†’ French translator (Gemma-3.4B, merged).\")\n",
    "print(\"Enter an English sentence. Type 'q' or 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    src = input(\"EN> \").strip()\n",
    "    if src.lower() in {\"q\", \"quit\"}:\n",
    "        print(\"Session terminated.\")\n",
    "        break\n",
    "    if not src:\n",
    "        continue\n",
    "    fr = translate_en_to_fr(src)\n",
    "    print(f\"FR> {fr}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
