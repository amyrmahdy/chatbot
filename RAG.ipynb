{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation with Quantized-Models\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using:\n",
    "- ChromaDB for vector storage and retrieval\n",
    "- Sentence Transformer for embedding generation\n",
    "- Llama3 for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "from bert_score import score\n",
    "from chromadb.utils import embedding_functions\n",
    "from transformers import logging as transformers_logging\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoModelForSeq2SeqLM, MT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define knowledge base chunks about NovaCloud service\n",
    "# context_data = {\n",
    "#     \"services\": '''\n",
    "# Ø´Ø±Ú©Øª Ù†ÙˆØ§Ú©Ù„ÙˆØ¯ Ø³Ù‡ Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:  \n",
    "# 1. Ù†ÙˆØ§Ø§Ø³ØªÙˆØ±Ø¬ (NovaStorage) â€“ ÛŒÚ© Ø³Ø±ÙˆÛŒØ³ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¨Ø±ÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ø§Ù…Ú©Ø§Ù† Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø³Ø±ØªØ§Ø³Ø±ÛŒ (E2EE) Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø±Ø§ Ø¯Ø§Ø±Ø¯.  \n",
    "# 2. Ù†ÙˆØ§Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± (NovaCompute) â€“ ÛŒÚ© Ø³Ø±ÙˆÛŒØ³ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø¨Ø±ÛŒ Ú©Ù‡ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ZetaCore X200 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø³Ù†Ú¯ÛŒÙ† Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.  \n",
    "# 3. Ù†ÙˆØ§Ú©Ø§Ù†Ú©Øª (NovaConnect) â€“ ÛŒÚ© Ù¾Ù„ØªÙØ±Ù… Ø´Ø¨Ú©Ù‡ Ø®ØµÙˆØµÛŒ Ø§Ø¨Ø±ÛŒ (VPC) Ú©Ù‡ Ø¨Ù‡ Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù† Ø§ÛŒØ¬Ø§Ø¯ Ø²ÛŒØ±Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ²ÙˆÙ„Ù‡ Ø¨Ø§ IP Ø«Ø§Ø¨Øª Ø®ØµÙˆØµÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.  \n",
    "# ''',\n",
    "#     \"pricing\": '''\n",
    "# Ù†ÙˆØ§Ú©Ù„ÙˆØ¯ Ø³Ù‡ Ø·Ø±Ø­ Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:  \n",
    "# - Ø·Ø±Ø­ Ù¾Ø§ÛŒÙ‡ (Basic): Ø´Ø§Ù…Ù„ ÛµÛ° Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª ÙØ¶Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Û² Ù‡Ø³ØªÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ÛŒ Ø¨Ø§ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÛŒ Û±Ûµ Ø¯Ù„Ø§Ø± Ø¯Ø± Ù…Ø§Ù‡  \n",
    "# - Ø·Ø±Ø­ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ (Pro): Ø´Ø§Ù…Ù„ ÛµÛ°Û° Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª ÙØ¶Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒØŒ Û¸ Ù‡Ø³ØªÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ÛŒ Ùˆ ØªØ±Ø§ÙÛŒÚ© Ù†Ø§Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ø§ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÛŒ Û¶Û° Ø¯Ù„Ø§Ø± Ø¯Ø± Ù…Ø§Ù‡  \n",
    "# - Ø·Ø±Ø­ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ (Enterprise): Ø´Ø§Ù…Ù„ Ûµ ØªØ±Ø§Ø¨Ø§ÛŒØª ÙØ¶Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒØŒ Û³Û² Ù‡Ø³ØªÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ÛŒØŒ Ùˆ Ù‚Ø§Ø¨Ù„ÛŒØª ØªÙ†Ø¸ÛŒÙ… Ø´Ø¨Ú©Ù‡ Ø®ØµÙˆØµÛŒ Ø§Ø®ØªØµØ§ØµÛŒ Ø¨Ø§ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÛŒ Û²Û°Û° Ø¯Ù„Ø§Ø± Ø¯Ø± Ù…Ø§Ù‡  \n",
    "# ''',\n",
    "#     \"security\": '''\n",
    "# Ù†ÙˆØ§Ú©Ù„ÙˆØ¯ Ø§Ù…Ù†ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ø³Ù‡ Ù…Ú©Ø§Ù†ÛŒØ²Ù… Ú©Ù„ÛŒØ¯ÛŒ ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯:  \n",
    "# - Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø³Ø±ØªØ§Ø³Ø±ÛŒ (E2EE) Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± NovaStorage  \n",
    "# - Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ú†Ù†Ø¯Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ (MFA) Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ ØªÙ…Ø§Ù…ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§  \n",
    "# - ÙØ§ÛŒØ±ÙˆØ§Ù„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ú©Ù‡ ØªÙ†Ù‡Ø§ IPÙ‡Ø§ÛŒ ØªØ£ÛŒÛŒØ¯â€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ Ø´Ø¨Ú©Ù‡ NovaConnect Ù…ØªØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯  \n",
    "# ''',\n",
    "#     \"uptime\": '''\n",
    "# Ø¯Ø± Ø³Ù‡ Ù…Ø§Ù‡ Ú¯Ø°Ø´ØªÙ‡ØŒ NovaCompute Ø¯Ø± Û¹Û¸.Û¹Ùª Ù…ÙˆØ§Ù‚Ø¹ Ø¨Ø¯ÙˆÙ† Ø§Ø®ØªÙ„Ø§Ù„ Ú©Ø§Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØŒ Ø§Ù…Ø§ ÛŒÚ© Ù‚Ø·Ø¹ÛŒ Û² Ø³Ø§Ø¹ØªÙ‡ Ø¯Ø± ØªØ§Ø±ÛŒØ® Û±Ûµ ÙÙˆØ±ÛŒÙ‡ Û²Û°Û²Û´ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±ÛŒ Ø±Ø® Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø¯Ø± Ù‡Ù…ÛŒÙ† Ù…Ø¯ØªØŒ NovaStorage Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† Ø§Ø®ØªÙ„Ø§Ù„ÛŒ ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª.  \n",
    "# '''\n",
    "# }\n",
    "\n",
    "# # Convert dictionary to list of chunks for embedding\n",
    "# chunks = list(context_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('endpoints_info.csv')\n",
    "\n",
    "df[['question','answer','context']].where(df['endpoint'] == 'ØªÙ…Ø§Ù… Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øª Ù‡Ø§').dropna()\n",
    "\n",
    "slice = df[['question','answer','context']].where(df['endpoint'] == 'ØªÙ…Ø§Ù… Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øª Ù‡Ø§').dropna()\n",
    "\n",
    "chunks = slice['context'].unique().tolist()\n",
    "questions = slice['question'].tolist()\n",
    "answers = slice['answer'].tolist()\n",
    "chunks_cooked = (slice['question'] + ' ' + slice['answer']).to_list()\n",
    "# (slice['question'] + ' ' + slice['context']).to_list()\n",
    "\n",
    "del slice\n",
    "del df\n",
    "\n",
    "# TODO: add chunks, questions and answers in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "def load_embedding_model(model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Load and configure the sentence transformer model for embeddings\"\"\"\n",
    "    embedding_model_path = f\"./models/{model_name}\"\n",
    "    \n",
    "    # Load model from local path\n",
    "    embedding_model = SentenceTransformer(embedding_model_path)\n",
    "    \n",
    "    # Create embedding function for ChromaDB\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    return embedding_model, sentence_transformer_ef\n",
    "\n",
    "# Initialize models\n",
    "embedding_model, sentence_transformer_ef = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vector_db(collection_name=\"novacloud_knowledge\", embedding_function=None):\n",
    "    \"\"\"Initialize ChromaDB and create or get collection\"\"\"\n",
    "    client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "    \n",
    "    # Delete the collection. Uncomment it if needed\n",
    "    client.delete_collection(collection_name)\n",
    "    # Get or create collection\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},  # Use cosine similarity for matching\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    return client, collection\n",
    "\n",
    "\n",
    "# Set up ChromaDB\n",
    "chroma_client, collection = setup_vector_db(collection_name = 'endpoints_info',embedding_function=sentence_transformer_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add Documents to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_collection(collection, documents, embedding_model):\n",
    "    \"\"\"Add documents to ChromaDB collection with embeddings\"\"\"\n",
    "    # Generate embeddings for documents\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    \n",
    "    # Add documents with embeddings to collection\n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        documents=documents,\n",
    "        ids=[str(i) for i in range(len(documents))]\n",
    "    )\n",
    "    \n",
    "    return len(documents)\n",
    "\n",
    "# Add documents to collection\n",
    "num_added = add_documents_to_collection(collection, chunks_cooked, embedding_model)\n",
    "print(f\"Added {num_added} documents to vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(query, collection, embedding_model, top_k=1, similarity_threshold=0.7):\n",
    "    \"\"\"Retrieve relevant documents based on semantic similarity\"\"\"\n",
    "    # Create embedding for query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    documents = results[\"documents\"][0] if results[\"documents\"] else [\"No relevant information found.\"]\n",
    "    distances = results[\"distances\"][0] if results[\"distances\"] else [1.0]  # Higher distance = less relevant\n",
    "    \n",
    "    # Print similarity scores for debugging\n",
    "    print(f\"Similarity scores: {[1-d for d in distances]}\")\n",
    "    \n",
    "    # Optional: Filter by similarity threshold\n",
    "    # filtered_docs = [doc for doc, dist in zip(documents, distances) if 1-dist >= similarity_threshold]\n",
    "    # return filtered_docs if filtered_docs else [\"No sufficiently relevant information found.\"]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test retrieval function\n",
    "top_k = 3\n",
    "test_query = questions[0]\n",
    "print(test_query)\n",
    "retrieved_docs = retrieve_relevant_documents(test_query, collection, embedding_model,top_k)\n",
    "print(retrieved_docs)\n",
    "for retrieved_docs in retrieved_docs:\n",
    "    print(f\"Retrieved document: {retrieved_docs[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load LLM for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(model_path=\"./models/Dorna-Llama3-8B-Instruct-GGUF-Q8/dorna-llama3-8b-instruct.Q8_0.gguf\",chat_format='auto'):\n",
    "    \"\"\"Load and configure the LLM for text generation\"\"\"\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        chat_format=chat_format,\n",
    "        n_gpu_layers=-1,  # Use all available GPU layers\n",
    "        n_ctx=9216,       # Context window size\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "\n",
    "# Define LLM models details like: path and chat_format\n",
    "llm_models_details = {\n",
    "    'dorna-llama3-8b-q8' : {'path': './models/Dorna-Llama3-8B-Instruct-GGUF-Q8/dorna-llama3-8b-instruct.Q8_0.gguf',\n",
    "                            'chat_format': 'llama-3'},\n",
    "    'deepseek-r1-7b-qwen' : {'path': './models/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B.Q8_0.gguf',\n",
    "                            'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-q2': {'path':'./models/gemma-3-4b-it.Q2_K/gemma-3-4b-it.Q2_K.gguf',\n",
    "                     'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-q8': {'path':'./models/gemma-3-4b-it.Q8_0/gemma-3-4b-it.Q8_0.gguf',\n",
    "                      'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-fp16': {'path':'./models/gemma-3-4b-it.fp16/gemma-3-4b-it.fp16.gguf',\n",
    "                        'chat_format': 'gemma'}\n",
    "}\n",
    "\n",
    "# Load Llama model\n",
    "target_llm_model = 'gemma-3-4b-fp16'\n",
    "llm_model_path, llm_chat_format = llm_models_details[target_llm_model]['path'], llm_models_details[target_llm_model]['chat_format']\n",
    "llm = load_llm_model(llm_model_path, llm_chat_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load NLP model for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_summarization_model(model_path=\"./models/mt5-persian-summary/\"):\n",
    "#     \"\"\"Load and configure the MT5 model for Persian summarization\"\"\"    \n",
    "#     # Initialize components\n",
    "#     tokenizer = MT5Tokenizer.from_pretrained(\n",
    "#         model_path,\n",
    "#         local_files_only=True,\n",
    "#         legacy=False,\n",
    "#         use_fast=True\n",
    "#     )\n",
    "    \n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#         model_path,\n",
    "#         local_files_only=True\n",
    "#     ).to(device)\n",
    "    \n",
    "#     return model, tokenizer, device\n",
    "\n",
    "# # Define available summarization models\n",
    "# summarization_models = {\n",
    "#     'mt5-persian-base': {\n",
    "#         'path': './models/mt5-persian-summary/',\n",
    "#         'description': 'Base MT5 model fine-tuned for Persian summarization'\n",
    "#     },\n",
    "#     'parst5-summary': {\n",
    "#         'path': './models/parsT5-base/',\n",
    "#         'description': 'Specialized Persian T5 for summarization'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Load model\n",
    "# target_model = 'mt5-persian-base'\n",
    "# summarizer_model, summarizer_tokenizer, summarizer_device = load_summarization_model(\n",
    "#     summarization_models[target_model]['path']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "PROMPT_TEMPLATE = '''\n",
    "ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª:\n",
    "{history}\n",
    "\n",
    "Ø¯Ø§Ù†Ø´ Ù¾Ø§ÛŒÙ‡:\n",
    "{context}\n",
    "\n",
    "Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:\n",
    "{prompt}\n",
    "'''\n",
    "\n",
    "# technical_support_system_message = \"ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù…ØªØ®ØµØµ Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ ÙÙ†ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³Ø±ÙˆÛŒØ³ Ù‡Ø§ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¯Ø§Ù†Ø´ Ù¾Ø§ÛŒÙ‡ØŒ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® ÙØ§Ø±Ø³ÛŒ Ù…ÛŒØ¯ÛŒ.\"\n",
    "technical_support_system_message = \"\"\"\n",
    "ğŸ”§ **Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ ÙÙ†ÛŒ** ğŸ”§\n",
    "\n",
    "ÙˆØ¸Ø§ÛŒÙ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:\n",
    "1. ØªØ®ØµØµ Ø§ØµÙ„ÛŒ: \n",
    "   - Ø§Ø±Ø§Ø¦Ù‡ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¯Ø±Ø¨Ø§Ø±Ù‡ ÙˆØ¶Ø¹ÛŒØª Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§\n",
    "   - Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ù…Ø´Ú©Ù„Ø§Øª Ø²ÛŒØ±Ø³Ø§Ø®ØªÛŒ (Ø´Ø¨Ú©Ù‡ØŒ Ø³Ø±ÙˆØ±ØŒ Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†)\n",
    "   - Ø§Ø·Ù„Ø§Ø¹â€ŒØ±Ø³Ø§Ù†ÛŒ Ù‚Ø·Ø¹ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù…Ø´Ú©Ù„Ø§Øª Ø¬Ø§Ø±ÛŒ\n",
    "\n",
    "2. Ø´ÛŒÙˆÙ‡ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ:\n",
    "   - Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø­Ø¯Ø§Ú©Ø«Ø± Ø¯Ø± 3 Ø¬Ù…Ù„Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø´ÙˆÙ†Ø¯\n",
    "   - Ø§Ø² Ø§ØµØ·Ù„Ø§Ø­Ø§Øª ÙÙ†ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯\n",
    "   - Ù…Ø´Ú©Ù„Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯\n",
    "\n",
    "3. Ù…Ù„Ø§Ø­Ø¸Ø§Øª:\n",
    "   - Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø§Ø·Ù…ÛŒÙ†Ø§Ù†ØŒ Ù¾Ø§Ø³Ø® Ø¯Ø§Ø¯Ù‡ Ù†Ø´ÙˆØ¯\n",
    "   - Ø§Ø² Ø¨ÛŒØ§Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­Ø±Ù…Ø§Ù†Ù‡ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú¯Ø±Ø¯Ø¯\n",
    "   - Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø­ÙˆØ²Ù‡ ÙÙ†ÛŒØŒ Ú©Ø§Ø±Ø¨Ø± Ø¨Ù‡ Ø¨Ø®Ø´ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø§Ø±Ø¬Ø§Ø¹ Ø´ÙˆØ¯\n",
    "\n",
    "Ù‚Ø§Ù„Ø¨ Ù¾Ø§Ø³Ø® Ø§ÛŒØ¯Ù‡â€ŒØ¢Ù„:\n",
    "âœ… ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ: [Ø´Ø±Ø­ Ù…Ø®ØªØµØ±]\n",
    "ğŸ” Ø¹Ù„Øª Ø§Ø­ØªÙ…Ø§Ù„ÛŒ: [ØªÙˆØ¶ÛŒØ­ ÙÙ†ÛŒ Ø³Ø§Ø¯Ù‡]\n",
    "ğŸ› ï¸ Ø±Ø§Ù‡Ú©Ø§Ø±: [Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©Ø§Ø±Ø¨Ø±/ØªÛŒÙ… ÙÙ†ÛŒ]\n",
    "\n",
    "Ù…Ø«Ø§Ù„:\n",
    "â¡ï¸ Ø³ÙˆØ§Ù„: \"Ú†Ø±Ø§ Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒÙ…ÛŒÙ„ Ú©Ø§Ø± Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ\"\n",
    "âš¡ Ù¾Ø§Ø³Ø®: \n",
    "ÙˆØ¶Ø¹ÛŒØª: Ù‚Ø·Ø¹ÛŒ Ù…ÙˆÙ‚Øª Ø¯Ø± Ø³Ø±ÙˆØ± Ø§ÛŒÙ…ÛŒÙ„ (Ø§Ø² 10:30)\n",
    "Ø¹Ù„Øª: Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±ÛŒ Ø¨Ø§ Ù…Ø´Ú©Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯\n",
    "Ø±Ø§Ù‡Ú©Ø§Ø±: Ø¯Ø± Ø­Ø§Ù„ Ø±ÙØ¹ - Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª Ø³Ø±ÙˆÛŒØ³: 12:00\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "summarizer_system_message = \"\"\"\n",
    "ØªÙˆ ÛŒÚ© Ø®Ù„Ø§ØµÙ‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù…ØªÙˆÙ† Ù‡Ø³ØªÛŒ. ÙˆØ¸ÛŒÙÙ‡ ØªÙˆ ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø®Ù„Ø§ØµÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø³Øª.\n",
    "- Ù‡Ø±Ú¯ÙˆÙ†Ù‡ Ø³Ù„Ø§Ù… Ùˆ Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ Ø±Ø§ Ø­Ø°Ù Ú©Ù†\n",
    "- ÙÙ‚Ø· Ù‡Ø³ØªÙ‡ Ø§ØµÙ„ÛŒ Ù…ØªÙ† Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±\n",
    "- Ø§Ú¯Ø± Ù…ØªÙ† Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª (Ú©Ù…ØªØ± Ø§Ø² 10 Ú©Ù„Ù…Ù‡)ØŒ Ø¹ÛŒÙ†Ø§Ù‹ ØªÚ©Ø±Ø§Ø±Ø´ Ú©Ù†\n",
    "- ØªØ­Øª Ù‡ÛŒÚ† Ø´Ø±Ø§ÛŒØ·ÛŒ Ø¨Ù‡ Ù…ØªÙ† Ù¾Ø§Ø³Ø® Ù†Ø¯Ù‡\n",
    "- ÙÙ‚Ø· Ù…ØªÙ† Ø®Ù„Ø§ØµÙ‡ Ø´Ø¯Ù‡ ÛŒØ§ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "def summarize_query_with_llm(query,llm=llm):\n",
    "    \"\"\"Summarize query using the LLM\"\"\"\n",
    "    prefix = 'Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ø®Ù„Ø§ØµÙ‡ Ú©Ù†:\\n'\n",
    "    query = prefix + query\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": summarizer_system_message},\n",
    "        {\"role\": \"user\", \"content\": \"Ø³Ù„Ø§Ù… Ø¯ÙˆØ³Øª Ø¹Ø²ÛŒØ²ØŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³ØªÙ… Ø¨Ù¾Ø±Ø³Ù… Ù…Ø¹Ù†ÛŒ Ú©Ø¯ ÛµÛ°Û² Ú†ÛŒÙ‡ØŸ\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Ù…Ø¹Ù†ÛŒ Ú©Ø¯ ÛµÛ°Û² Ú†ÛŒÙ‡ØŸ\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{query}\"}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    # .split('</think>')[-1] if the model thinks!\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    print(f\"Summarized version: {response_clean}\")\n",
    "\n",
    "    return response_clean\n",
    "    \n",
    "\n",
    "\n",
    "# def summarize_query_with_mt5(text, model, tokenizer, device, max_length=250):\n",
    "#     \"\"\"Generate summary using the loaded model\"\"\"\n",
    "#     # Persian-specific task prefix\n",
    "#     input_text = f\"Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ: {text}\"\n",
    "    \n",
    "#     # Tokenize and move to correct device\n",
    "#     inputs = tokenizer(\n",
    "#         input_text,\n",
    "#         return_tensors=\"pt\",\n",
    "#         max_length=512,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\"\n",
    "#     ).to(device)\n",
    "    \n",
    "#     # Generate with optimized parameters\n",
    "#     response = model.generate(\n",
    "#         input_ids=inputs.input_ids,\n",
    "#         attention_mask=inputs.attention_mask,\n",
    "#         max_length=max_length,\n",
    "#         num_beams=4,\n",
    "#         repetition_penalty=2.5,\n",
    "#         length_penalty=0.8,\n",
    "#         early_stopping=True,\n",
    "#         no_repeat_ngram_size=3\n",
    "#     )\n",
    "#     response_clean = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "#     print(f\"Summarized version: {response_clean}\")\n",
    "#     return response_clean\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_context(query, collection=collection, embedding_model=embedding_model,top_k=top_k):\n",
    "    \"\"\"Retrieve relevant context based on the query\"\"\"\n",
    "    docs = retrieve_relevant_documents(query, collection, embedding_model,top_k)\n",
    "    for doc in docs:\n",
    "        print(f\"Retrieved document: {doc[:100]}...\")\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "\n",
    "def generate_response_stream(model_input, llm=llm):\n",
    "    \"\"\"Generate streaming response using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": technical_support_system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"{model_input}\"}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1,  # Low temperature for more deterministic responses\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in response:\n",
    "        delta = chunk['choices'][0]['delta']\n",
    "        if 'content' in delta:\n",
    "            content = delta['content']\n",
    "            print(content, end='', flush=True)\n",
    "            full_response += content\n",
    "            yield content\n",
    "\n",
    "\n",
    "def generate_response(model_input, llm=llm):\n",
    "    \"\"\"Generate response using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": technical_support_system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"{model_input}\"}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    # .split('</think>')[-1] if the model thinks!\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    print(response_clean)\n",
    "\n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def rag_chat(user_query, history=None, stream=False, summary = False):\n",
    "    \"\"\"Complete RAG pipeline: Retrieve â†’ Generate â†’ Respond\"\"\"\n",
    "    if history is None:\n",
    "        history = conversation_history\n",
    "    \n",
    "    # Format conversation history\n",
    "    history_text = \"\\n\".join(history)\n",
    "\n",
    "    if summary:\n",
    "        # Summarize input query\n",
    "        # With MT5\n",
    "        # user_query_summarized = summarize_query_with_mt5(user_query, summarizer_model, summarizer_tokenizer, summarizer_device)\n",
    "        # With LLM\n",
    "        user_query_summarized = summarize_query_with_llm(user_query)\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        context = retrieve_context(user_query_summarized)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        # Retrieve relevant context\n",
    "        context = retrieve_context(user_query)\n",
    "    \n",
    "    \n",
    "    # Create prompt with context and history\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        history=history_text,\n",
    "        context=context, \n",
    "        prompt=user_query\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        # Generate streaming response\n",
    "        response = \"\"\n",
    "        response_stream = generate_response_stream(prompt)\n",
    "        for chunk in response_stream:\n",
    "            response += chunk\n",
    "    else:\n",
    "        # Generate response (non-stream)\n",
    "        response = generate_response(prompt)\n",
    "\n",
    "    if summary:\n",
    "        # Summarize ouput query \n",
    "        # With MT5\n",
    "        # response_summarized = summarize_query_with_mt5(response, summarizer_model, summarizer_tokenizer, summarizer_device)\n",
    "        # With LLM\n",
    "        response_summarized = summarize_query_with_llm(response)\n",
    "        \n",
    "        # Update conversation history\n",
    "        history.append(f\"Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {user_query_summarized}\")\n",
    "        history.append(f\"Ù¾Ø§Ø³Ø® Ú©Ù…Ú© Ú©Ù†Ù†Ø¯Ù‡: {response_summarized}\")\n",
    "\n",
    "    else:\n",
    "        # Update conversation history\n",
    "        history.append(f\"Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {user_query}\")\n",
    "        history.append(f\"Ù¾Ø§Ø³Ø® Ú©Ù…Ú© Ú©Ù†Ù†Ø¯Ù‡: {response}\")\n",
    "    \n",
    "    llm.reset()\n",
    "    return response,context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic question (Without Summary)\n",
    "query1 = questions[0]\n",
    "print(f\"User query: {query1}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query1,stream = True, summary=False)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic question (With Summary)\n",
    "query1 = questions[0]\n",
    "print(f\"User query: {query1}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query1,stream = True, summary=True)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Follow-up question\n",
    "query2 = \"Ù…ÛŒØ´Ù‡ Ø¨ÛŒØ´ØªØ± Ø±Ø§Ø¬Ø¹ Ø¨Ù‡ Ø§ÛŒÙ† ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯ÛŒØŸ\"\n",
    "print(f\"User query: {query2}\")\n",
    "\n",
    "# Time the response (using existing conversation history)\n",
    "start = time.time()\n",
    "response, context = rag_chat(query2,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Different topic question\n",
    "query3 = questions[2]\n",
    "print(f\"User query: {query3}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query3,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. RAG System Evaluation\n",
    "\n",
    "Test with more complex queries to evaluate retrieval performance and answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_response_cosine(generated_response, ground_truth_answer,embedding_model=embedding_model):\n",
    "    \n",
    "    \"\"\"Evaluation for generated response by model vs ground truth answer\"\"\"\n",
    "    generated_response_embeddings = embedding_model.encode(generated_response, convert_to_tensor=True)\n",
    "    ground_truth_answer_embeddings = embedding_model.encode(ground_truth_answer, convert_to_tensor=True)\n",
    "\n",
    "    cosine_score_raw = util.pytorch_cos_sim(generated_response_embeddings, ground_truth_answer_embeddings)\n",
    "    \n",
    "    cosine_score = round(float(cosine_score_raw[0][0]) * 100, 2)\n",
    "    print(\"Cosine Similarity between generated response and ground truth answer:\", cosine_score)\n",
    "    \n",
    "    return cosine_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_response_prf(generated_response, ground_truth_answer):\n",
    "    \n",
    "    \"\"\"Evaluation for generated response by model vs ground truth answer\"\"\"\n",
    "\n",
    "    P_raw, R_raw, F1_raw = score([generated_response], [ground_truth_answer], lang='en') # model_type='distilbert-base-uncased'\n",
    "    P = round(float(P_raw[0]) * 100, 2)\n",
    "    R = round(float(R_raw[0]) * 100, 2)\n",
    "    F1 = round(float(F1_raw[0]) * 100, 2)\n",
    "    print(\"Precision: \", P)\n",
    "    print(\"Recall: \", R)\n",
    "    print(\"F1 Score: \", F1)\n",
    "    \n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_retrieved_context_cosine(retrieved_context, ground_truth_context,embedding_model=embedding_model):\n",
    "#     \"\"\"Evaluation for retrieval quality; retrieved contexts vs ground truth context\"\"\"\n",
    "#     # TODO: It is just the sample to remember. Make it correct later\n",
    "\n",
    "#     retrieved_context_embeddings = embedding_model.encode(retrieved_context, convert_to_tensor=True)\n",
    "#     ground_truth_context_embeddings = embedding_model.encode(ground_truth_context, convert_to_tensor=True)\n",
    "\n",
    "#     cosine_score = util.pytorch_cos_sim(retrieved_context_embeddings, ground_truth_context_embeddings)\n",
    "\n",
    "#     print(\"Cosine Similarity between generated response and ground truth answer:\", cosine_score)\n",
    "    \n",
    "#     return cosine_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat_with_processing_time(query):\n",
    "    \"\"\"RAG chat + processing time \"\"\"    \n",
    "    print(f\"User query: {query}\")\n",
    "\n",
    "    # Reset conversation history\n",
    "    conversation_history = []\n",
    "\n",
    "    # Time the response\n",
    "    start = time.time()\n",
    "    response, context = rag_chat(query,stream=True)\n",
    "    end = time.time()\n",
    "\n",
    "    processing_time = f\"{end - start:.2f}\"\n",
    "    print(f\"---\\nProcessing time: {processing_time} seconds\")\n",
    "    return response, context, processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(questions=questions,answers=answers):\n",
    "    reports = {\n",
    "        'question': [],\n",
    "        'response': [],\n",
    "        'answer': [],\n",
    "        'cosine': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'context': [],\n",
    "        'processing_time': []\n",
    "    }\n",
    "    for question,answer in zip(questions,answers):\n",
    "        response, context, processing_time = rag_chat_with_processing_time(question)\n",
    "        cosine = evaluate_generated_response_cosine(response, answer,embedding_model=embedding_model)\n",
    "        precision, recall, f1_score = evaluate_generated_response_prf(response, answer)\n",
    "        reports['question'].append(question)\n",
    "        reports['response'].append(response)\n",
    "        reports['answer'].append(answer)\n",
    "        reports['cosine'].append(cosine)\n",
    "        reports['precision'].append(precision)\n",
    "        reports['recall'].append(recall)\n",
    "        reports['f1_score'].append(f1_score)\n",
    "        reports['context'].append(context)\n",
    "        reports['processing_time'].append(processing_time)\n",
    "        print(\"======================================\")\n",
    "        print(\"======================================\")\n",
    "    return reports\n",
    "\n",
    "def save_report_to_csv(reports):\n",
    "    df_reports = pd.DataFrame(reports)\n",
    "    df_reports.to_csv('report.csv', index=False)\n",
    "    print(\"Report saved to the file successfully.\")\n",
    "    return 0\n",
    "\n",
    "reports = generate_report()\n",
    "save_report_to_csv(reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
