{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation with Quantized-Models\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using:\n",
    "- ChromaDB for vector storage and retrieval\n",
    "- Sentence Transformer for embedding generation\n",
    "- Llama3 for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import random\n",
    "import chromadb\n",
    "from llama_cpp import Llama\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define knowledge base chunks about NovaCloud service\n",
    "context_data = {\n",
    "    \"services\": '''\n",
    "شرکت نواکلود سه سرویس اصلی ارائه می‌دهد:  \n",
    "1. نوااستورج (NovaStorage) – یک سرویس ذخیره‌سازی ابری که برای شرکت‌های بزرگ طراحی شده است و امکان رمزگذاری سرتاسری (E2EE) و پشتیبان‌گیری خودکار را دارد.  \n",
    "2. نواکامپیوتر (NovaCompute) – یک سرویس پردازش ابری که از پردازنده‌های ZetaCore X200 استفاده می‌کند و برای مدل‌های یادگیری ماشین سنگین بهینه‌سازی شده است.  \n",
    "3. نواکانکت (NovaConnect) – یک پلتفرم شبکه خصوصی ابری (VPC) که به شرکت‌ها امکان ایجاد زیرشبکه‌های ایزوله با IP ثابت خصوصی را می‌دهد.  \n",
    "''',\n",
    "    \"pricing\": '''\n",
    "نواکلود سه طرح قیمت‌گذاری ارائه می‌دهد:  \n",
    "- طرح پایه (Basic): شامل ۵۰ گیگابایت فضای ذخیره‌سازی و ۲ هسته پردازشی با هزینه‌ی ۱۵ دلار در ماه  \n",
    "- طرح حرفه‌ای (Pro): شامل ۵۰۰ گیگابایت فضای ذخیره‌سازی، ۸ هسته پردازشی و ترافیک نامحدود با هزینه‌ی ۶۰ دلار در ماه  \n",
    "- طرح سازمانی (Enterprise): شامل ۵ ترابایت فضای ذخیره‌سازی، ۳۲ هسته پردازشی، و قابلیت تنظیم شبکه خصوصی اختصاصی با هزینه‌ی ۲۰۰ دلار در ماه  \n",
    "''',\n",
    "    \"security\": '''\n",
    "نواکلود امنیت داده‌ها را با سه مکانیزم کلیدی تضمین می‌کند:  \n",
    "- رمزگذاری سرتاسری (E2EE) برای داده‌های ذخیره‌شده در NovaStorage  \n",
    "- احراز هویت چندمرحله‌ای (MFA) برای ورود به تمامی سرویس‌ها  \n",
    "- فایروال هوشمند که تنها IPهای تأیید‌شده را به شبکه NovaConnect متصل می‌کند  \n",
    "''',\n",
    "    \"uptime\": '''\n",
    "در سه ماه گذشته، NovaCompute در ۹۸.۹٪ مواقع بدون اختلال کار کرده است، اما یک قطعی ۲ ساعته در تاریخ ۱۵ فوریه ۲۰۲۴ به دلیل بروزرسانی سخت‌افزاری رخ داده است. در همین مدت، NovaStorage بدون هیچ اختلالی فعال بوده است.  \n",
    "'''\n",
    "}\n",
    "\n",
    "# Convert dictionary to list of chunks for embedding\n",
    "chunks = list(context_data.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "def load_embedding_model(model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Load and configure the sentence transformer model for embeddings\"\"\"\n",
    "    embedding_model_path = f\"./models/{model_name}\"\n",
    "    \n",
    "    # Load model from local path\n",
    "    embedding_model = SentenceTransformer(embedding_model_path)\n",
    "    \n",
    "    # Create embedding function for ChromaDB\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    return embedding_model, sentence_transformer_ef\n",
    "\n",
    "# Initialize models\n",
    "embedding_model, sentence_transformer_ef = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vector_db(collection_name=\"novacloud_knowledge\", embedding_function=None):\n",
    "    \"\"\"Initialize ChromaDB and create or get collection\"\"\"\n",
    "    client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "    \n",
    "    # Get or create collection\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},  # Use cosine similarity for matching\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    return client, collection\n",
    "\n",
    "# Set up ChromaDB\n",
    "chroma_client, collection = setup_vector_db(embedding_function=sentence_transformer_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add Documents to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_collection(collection, documents, embedding_model):\n",
    "    \"\"\"Add documents to ChromaDB collection with embeddings\"\"\"\n",
    "    # Generate embeddings for documents\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    \n",
    "    # Add documents with embeddings to collection\n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        documents=documents,\n",
    "        ids=[str(i) for i in range(len(documents))]\n",
    "    )\n",
    "    \n",
    "    return len(documents)\n",
    "\n",
    "# Add documents to collection\n",
    "num_added = add_documents_to_collection(collection, chunks, embedding_model)\n",
    "print(f\"Added {num_added} documents to vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(query, collection, embedding_model, top_k=1, similarity_threshold=0.7):\n",
    "    \"\"\"Retrieve relevant documents based on semantic similarity\"\"\"\n",
    "    # Create embedding for query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    documents = results[\"documents\"][0] if results[\"documents\"] else [\"No relevant information found.\"]\n",
    "    distances = results[\"distances\"][0] if results[\"distances\"] else [1.0]  # Higher distance = less relevant\n",
    "    \n",
    "    # Print similarity scores for debugging\n",
    "    print(f\"Similarity scores: {[1-d for d in distances]}\")\n",
    "    \n",
    "    # Optional: Filter by similarity threshold\n",
    "    # filtered_docs = [doc for doc, dist in zip(documents, distances) if 1-dist >= similarity_threshold]\n",
    "    # return filtered_docs if filtered_docs else [\"No sufficiently relevant information found.\"]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test retrieval function\n",
    "test_query = \"کدام سرویس نواکلود برای ذخیره‌سازی ابری استفاده می‌شود؟\"\n",
    "retrieved_docs = retrieve_relevant_documents(test_query, collection, embedding_model)\n",
    "print(f\"Retrieved document: {retrieved_docs[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load LLM for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(model_path=\"./models/Dorna-Llama3-8B-Instruct-GGUF-Q8/dorna-llama3-8b-instruct.Q8_0.gguf\"):\n",
    "    \"\"\"Load and configure the LLM for text generation\"\"\"\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        chat_format=\"llama-3\",\n",
    "        n_gpu_layers=-1,  # Use all available GPU layers\n",
    "        n_ctx=2048,       # Context window size\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Load Llama model\n",
    "llm = load_llm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "PROMPT_TEMPLATE = '''\n",
    "تاریخچه مکالمات:\n",
    "{history}\n",
    "\n",
    "دانش پایه:\n",
    "{context}\n",
    "\n",
    "سوال کاربر:\n",
    "{prompt}\n",
    "'''\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def summarize_query(query,llm=llm):\n",
    "    \"\"\"Summarize query using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"تو یک دستیار هستی که وظیفه تو خلاصه کردن متن است. سعی نکن به کاربر جواب بدی فقط تشریفات رو از بین ببر و تا جایی که امکان داره خلاصه کن. فقط تا جایی پرامپت کاربر رو کوتاه کن که به هسته اصلی مطلب آسیبی وارد نشه.\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    print(f\"Summarized version: {response['choices'][0]['message']['content']}\")\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def retrieve_context(query, collection=collection, embedding_model=embedding_model):\n",
    "    \"\"\"Retrieve relevant context based on the query\"\"\"\n",
    "    docs = retrieve_relevant_documents(query, collection, embedding_model)\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "def generate_response(model_input, llm=llm):\n",
    "    \"\"\"Generate response using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"تو یک دستیار متخصص و پشتیبانی فنی وضعیت سرویس ها هستی که با توجه به دانش پایه، به کاربر پاسخ کمک کننده میدی\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{model_input}\"}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def rag_chat(user_query, history=None):\n",
    "    \"\"\"Complete RAG pipeline: Retrieve → Generate → Respond\"\"\"\n",
    "    if history is None:\n",
    "        history = conversation_history\n",
    "    \n",
    "    # Summarize input query \n",
    "    # user_query_summarized = summarize_query(user_query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(user_query)\n",
    "    \n",
    "    # Format conversation history\n",
    "    history_text = \"\\n\".join(history)\n",
    "    \n",
    "    # Create prompt with context and history\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        history=history_text,\n",
    "        context=context, \n",
    "        prompt=user_query\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response(prompt)\n",
    "    \n",
    "    # Summarize ouput query \n",
    "    # response_summarized = summarize_query(response)\n",
    "\n",
    "\n",
    "    # Update conversation history\n",
    "    history.append(f\"سوال کاربر: {user_query}\")\n",
    "    history.append(f\"پاسخ کمک کننده: {response}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Streaming Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_text(text, min_delay=0.02, max_delay=0.08):\n",
    "    \"\"\"Display text in a streaming manner, character by character\"\"\"\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Dynamic delay for natural effect\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    print()  # New line after completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic question\n",
    "query1 = \"کدام سرویس نواکلود برای ذخیره‌سازی ابری استفاده می‌شود؟\"\n",
    "print(f\"User query: {query1}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response = rag_chat(query1)\n",
    "end = time.time()\n",
    "\n",
    "# Stream or print the response\n",
    "# stream_option = input(\"Stream response? (y/n): \").lower().strip() == 'y'\n",
    "stream_option = False\n",
    "if stream_option:\n",
    "    stream_text(response)\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Follow-up question\n",
    "query2 = \"میشه بیشتر راجع به این سرویس توضیح بدی؟\"\n",
    "print(f\"User query: {query2}\")\n",
    "\n",
    "# Time the response (using existing conversation history)\n",
    "start = time.time()\n",
    "response = rag_chat(query2)\n",
    "end = time.time()\n",
    "\n",
    "# Stream or print the response\n",
    "if stream_option:\n",
    "    stream_text(response)\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Different topic question\n",
    "query3 = '''\n",
    "سلام و عرض ادب\n",
    "وقت شما بخیر\n",
    "من یک سوالی داشتم ازتون\n",
    "امکانش هست بفرمایید که در طرح Pro چند هسته پردازشی ارائه می‌شود؟\n",
    "ممنون از شما\n",
    "'''\n",
    "print(f\"User query: {query3}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response = rag_chat(query3)\n",
    "end = time.time()\n",
    "\n",
    "# Stream or print the response\n",
    "if stream_option:\n",
    "    stream_text(response)\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query3 = '''\n",
    "سلام و عرض ادب\n",
    "وقت شما بخیر\n",
    "من یک سوالی داشتم ازتون\n",
    "امکانش هست بفرمایید که در طرح Pro چند هسته پردازشی ارائه می‌شود؟\n",
    "ممنون از شما\n",
    "'''\n",
    "print(f\"User query: {query3}\")\n",
    "\n",
    "\n",
    "\"\"\"Summarize query using the LLM\"\"\"\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"تو یک دستیار هستی که وظیفه تو خلاصه کردن متن است. سعی نکن به کاربر جواب بدی فقط تشریفات رو از بین ببر و تا جایی که امکان داره خلاصه کن. فقط تا جایی پرامپت کاربر رو کوتاه کن که به هسته اصلی مطلب آسیبی وارد نشه.\"},\n",
    "    {\"role\": \"system\", \"content\": \"خلاصه کن, خلاصه کن, خلاصه کن, خلاصه کن, خلاصه کن\"},\n",
    "    {\"role\": \"user\", \"content\": query3},\n",
    "]\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    top_p=0.85,\n",
    "    temperature=0.1  # Low temperature for more deterministic responses\n",
    ")\n",
    "print(f\"Summarized version: {response['choices'][0]['message']['content']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. RAG System Evaluation\n",
    "\n",
    "Test with more complex queries to evaluate retrieval performance and answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add evaluation functions here if needed\n",
    "def evaluate_retrieval(test_queries, ground_truth_answers):\n",
    "    \"\"\"Simple evaluation for retrieval quality\"\"\"\n",
    "    # TODO: Implement evaluation metrics\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amyrmahdy/GitHub/chatbot/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/amyrmahdy/GitHub/chatbot/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  tensor([0.9786])\n",
      "Recall:  tensor([0.9562])\n",
      "F1 Score:  tensor([0.9672])\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "generated_response = \"Paris is the capital of France and has a population of 5 million people. The Eiffel Tower was completed in 1889.\"\n",
    "reference_docs = \"Paris is the capital of France. It has a population of over 2 million people. The Eiffel Tower is a famous landmark in Paris, completed in 1889.\"\n",
    "\n",
    "P, R, F1 = score([generated_response], [reference_docs], lang='en')\n",
    "\n",
    "print(\"Precision: \", P)\n",
    "print(\"Recall: \", R)\n",
    "print(\"F1 Score: \", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between generated and retrieved texts: tensor([[0.8000, 0.7517]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('./models/all-MiniLM-L6-v2')\n",
    "\n",
    "retrieved_text = [\"Paris is the capital of France. It has a population of over 2 million people.\",\n",
    "                  \"The Eiffel Tower is a famous landmark in Paris, completed in 1889.\"]\n",
    "generated_text = \"Paris is the capital of France and has a population of 5 million people. The Eiffel Tower was completed in 1889.\"\n",
    "\n",
    "retrieved_embeddings = model.encode(retrieved_text, convert_to_tensor=True)\n",
    "generated_embedding = model.encode(generated_text, convert_to_tensor=True)\n",
    "\n",
    "cosine_scores = util.pytorch_cos_sim(generated_embedding, retrieved_embeddings)\n",
    "\n",
    "print(\"Cosine Similarity between generated and retrieved texts:\", cosine_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
