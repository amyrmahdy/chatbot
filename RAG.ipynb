{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation with Dorna-Llama3-8B-Instruct-Quantized\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using:\n",
    "- ChromaDB for vector storage and retrieval\n",
    "- Sentence Transformer for embedding generation\n",
    "- Llama3 for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amyrmahdy/GitHub/chatbot/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import random\n",
    "import chromadb\n",
    "from llama_cpp import Llama\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define knowledge base chunks about NovaCloud service\n",
    "context_data = {\n",
    "    \"services\": '''\n",
    "شرکت نواکلود سه سرویس اصلی ارائه می‌دهد:  \n",
    "1. نوااستورج (NovaStorage) – یک سرویس ذخیره‌سازی ابری که برای شرکت‌های بزرگ طراحی شده است و امکان رمزگذاری سرتاسری (E2EE) و پشتیبان‌گیری خودکار را دارد.  \n",
    "2. نواکامپیوتر (NovaCompute) – یک سرویس پردازش ابری که از پردازنده‌های ZetaCore X200 استفاده می‌کند و برای مدل‌های یادگیری ماشین سنگین بهینه‌سازی شده است.  \n",
    "3. نواکانکت (NovaConnect) – یک پلتفرم شبکه خصوصی ابری (VPC) که به شرکت‌ها امکان ایجاد زیرشبکه‌های ایزوله با IP ثابت خصوصی را می‌دهد.  \n",
    "''',\n",
    "    \"pricing\": '''\n",
    "نواکلود سه طرح قیمت‌گذاری ارائه می‌دهد:  \n",
    "- طرح پایه (Basic): شامل ۵۰ گیگابایت فضای ذخیره‌سازی و ۲ هسته پردازشی با هزینه‌ی ۱۵ دلار در ماه  \n",
    "- طرح حرفه‌ای (Pro): شامل ۵۰۰ گیگابایت فضای ذخیره‌سازی، ۸ هسته پردازشی و ترافیک نامحدود با هزینه‌ی ۶۰ دلار در ماه  \n",
    "- طرح سازمانی (Enterprise): شامل ۵ ترابایت فضای ذخیره‌سازی، ۳۲ هسته پردازشی، و قابلیت تنظیم شبکه خصوصی اختصاصی با هزینه‌ی ۲۰۰ دلار در ماه  \n",
    "''',\n",
    "    \"security\": '''\n",
    "نواکلود امنیت داده‌ها را با سه مکانیزم کلیدی تضمین می‌کند:  \n",
    "- رمزگذاری سرتاسری (E2EE) برای داده‌های ذخیره‌شده در NovaStorage  \n",
    "- احراز هویت چندمرحله‌ای (MFA) برای ورود به تمامی سرویس‌ها  \n",
    "- فایروال هوشمند که تنها IPهای تأیید‌شده را به شبکه NovaConnect متصل می‌کند  \n",
    "''',\n",
    "    \"uptime\": '''\n",
    "در سه ماه گذشته، NovaCompute در ۹۸.۹٪ مواقع بدون اختلال کار کرده است، اما یک قطعی ۲ ساعته در تاریخ ۱۵ فوریه ۲۰۲۴ به دلیل بروزرسانی سخت‌افزاری رخ داده است. در همین مدت، NovaStorage بدون هیچ اختلالی فعال بوده است.  \n",
    "'''\n",
    "}\n",
    "\n",
    "# Convert dictionary to list of chunks for embedding\n",
    "chunks = list(context_data.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "def load_embedding_model(model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Load and configure the sentence transformer model for embeddings\"\"\"\n",
    "    embedding_model_path = f\"./models/{model_name}\"\n",
    "    \n",
    "    # Load model from local path\n",
    "    embedding_model = SentenceTransformer(embedding_model_path)\n",
    "    \n",
    "    # Create embedding function for ChromaDB\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    return embedding_model, sentence_transformer_ef\n",
    "\n",
    "# Initialize models\n",
    "embedding_model, sentence_transformer_ef = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vector_db(collection_name=\"novacloud_knowledge\", embedding_function=None):\n",
    "    \"\"\"Initialize ChromaDB and create or get collection\"\"\"\n",
    "    client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "    \n",
    "    # Get or create collection\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},  # Use cosine similarity for matching\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    return client, collection\n",
    "\n",
    "# Set up ChromaDB\n",
    "chroma_client, collection = setup_vector_db(embedding_function=sentence_transformer_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add Documents to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 4 documents to vector database\n"
     ]
    }
   ],
   "source": [
    "def add_documents_to_collection(collection, documents, embedding_model):\n",
    "    \"\"\"Add documents to ChromaDB collection with embeddings\"\"\"\n",
    "    # Generate embeddings for documents\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    \n",
    "    # Add documents with embeddings to collection\n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        documents=documents,\n",
    "        ids=[str(i) for i in range(len(documents))]\n",
    "    )\n",
    "    \n",
    "    return len(documents)\n",
    "\n",
    "# Add documents to collection\n",
    "num_added = add_documents_to_collection(collection, chunks, embedding_model)\n",
    "print(f\"Added {num_added} documents to vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores: [0.5962998214527002]\n",
      "Retrieved document: \n",
      "نواکلود امنیت داده‌ها را با سه مکانیزم کلیدی تضمین می‌کند:  \n",
      "- رمزگذاری سرتاسری (E2EE) برای داده‌ها...\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_documents(query, collection, embedding_model, top_k=1, similarity_threshold=0.7):\n",
    "    \"\"\"Retrieve relevant documents based on semantic similarity\"\"\"\n",
    "    # Create embedding for query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    documents = results[\"documents\"][0] if results[\"documents\"] else [\"No relevant information found.\"]\n",
    "    distances = results[\"distances\"][0] if results[\"distances\"] else [1.0]  # Higher distance = less relevant\n",
    "    \n",
    "    # Print similarity scores for debugging\n",
    "    print(f\"Similarity scores: {[1-d for d in distances]}\")\n",
    "    \n",
    "    # Optional: Filter by similarity threshold\n",
    "    # filtered_docs = [doc for doc, dist in zip(documents, distances) if 1-dist >= similarity_threshold]\n",
    "    # return filtered_docs if filtered_docs else [\"No sufficiently relevant information found.\"]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test retrieval function\n",
    "test_query = \"کدام سرویس نواکلود برای ذخیره‌سازی ابری استفاده می‌شود؟\"\n",
    "retrieved_docs = retrieve_relevant_documents(test_query, collection, embedding_model)\n",
    "print(f\"Retrieved document: {retrieved_docs[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load LLM for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(model_path=\"./models/Dorna-Llama3-8B-Instruct-GGUF-Q8/dorna-llama3-8b-instruct.Q8_0.gguf\"):\n",
    "    \"\"\"Load and configure the LLM for text generation\"\"\"\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        chat_format=\"llama-3\",\n",
    "        n_gpu_layers=-1,  # Use all available GPU layers\n",
    "        n_ctx=2048,       # Context window size\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Load Llama model\n",
    "llm = load_llm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "PROMPT_TEMPLATE = '''\n",
    "تاریخچه مکالمات:\n",
    "{history}\n",
    "\n",
    "دانش پایه:\n",
    "{context}\n",
    "\n",
    "سوال کاربر:\n",
    "{prompt}\n",
    "'''\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def summarize_query(query,llm=llm):\n",
    "    \"\"\"Summarize query using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"تو یک دستیار هستی که وظیفه تو خلاصه کردن متن است. سعی نکن به کاربر جواب بدی فقط تشریفات رو از بین ببر و تا جایی که امکان داره خلاصه کن. فقط تا جایی پرامپت کاربر رو کوتاه کن که به هسته اصلی مطلب آسیبی وارد نشه.\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    print(f\"Summarized version: {response['choices'][0]['message']['content']}\")\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def retrieve_context(query, collection=collection, embedding_model=embedding_model):\n",
    "    \"\"\"Retrieve relevant context based on the query\"\"\"\n",
    "    docs = retrieve_relevant_documents(query, collection, embedding_model)\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "def generate_response(model_input, llm=llm):\n",
    "    \"\"\"Generate response using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"تو یک دستیار متخصص و پشتیبانی فنی وضعیت سرویس ها هستی که با توجه به دانش پایه، به کاربر پاسخ کمک کننده میدی\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{model_input}\"}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def rag_chat(user_query, history=None):\n",
    "    \"\"\"Complete RAG pipeline: Retrieve → Generate → Respond\"\"\"\n",
    "    if history is None:\n",
    "        history = conversation_history\n",
    "    \n",
    "    # Summarize input query \n",
    "    # user_query_summarized = summarize_query(user_query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(user_query)\n",
    "    \n",
    "    # Format conversation history\n",
    "    history_text = \"\\n\".join(history)\n",
    "    \n",
    "    # Create prompt with context and history\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        history=history_text,\n",
    "        context=context, \n",
    "        prompt=user_query\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response(prompt)\n",
    "    \n",
    "    # Summarize ouput query \n",
    "    # response_summarized = summarize_query(response)\n",
    "\n",
    "\n",
    "    # Update conversation history\n",
    "    history.append(f\"سوال کاربر: {user_query}\")\n",
    "    history.append(f\"پاسخ کمک کننده: {response}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Streaming Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_text(text, min_delay=0.02, max_delay=0.08):\n",
    "    \"\"\"Display text in a streaming manner, character by character\"\"\"\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Dynamic delay for natural effect\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    print()  # New line after completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: کدام سرویس نواکلود برای ذخیره‌سازی ابری استفاده می‌شود؟\n",
      "Similarity scores: [0.5962998214527002]\n",
      "بر اساس دانش پایه، سرویس NovaStorage برای ذخیره‌سازی ابری استفاده می‌شود. این سرویس با استفاده از رمزگذاری سرتاسری (E2EE) برای داده‌های ذخیره‌شده تضمین می‌کند.\n",
      "---\n",
      "Processing time: 1.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic question\n",
    "query1 = \"کدام سرویس نواکلود برای ذخیره‌سازی ابری استفاده می‌شود؟\"\n",
    "print(f\"User query: {query1}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response = rag_chat(query1)\n",
    "end = time.time()\n",
    "\n",
    "# Stream or print the response\n",
    "stream_option = input(\"Stream response? (y/n): \").lower().strip() == 'y'\n",
    "if stream_option:\n",
    "    stream_text(response)\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: میشه بیشتر راجع به این سرویس توضیح بدی؟\n",
      "Similarity scores: [0.608187841441516]\n",
      "با خوشحالی! به عنوان یک دستیار متخصص و پشتیبانی فنی، خوشحالم که می‌توانم به شما در مورد سرویس NovaStorage توضیح بدهم.\n",
      "\n",
      "نواستورج (NovaStorage) یک سرویس ذخیره‌سازی ابری است که توسط نواکلود ارائه می‌شود. این سرویس با استفاده از رمزگذاری سرتاسری (E2EE) برای داده‌های ذخیره‌شده تضمین می‌کند که داده‌های شما در دسترس دیگران قرار نخواهد گرفت. این رمزگذاری سرتاسری تضمین می‌کند که تنها شما و کسانی که به داده‌های شما دسترسی دارند، می‌توانند به آن‌ها دسترسی داشته باشند.\n",
      "\n",
      "نواستورج همچنین از یک سیستم احراز هویت چندمرحله‌ای (MFA) برای ورود به تمامی سرویس‌ها استفاده می‌کند. این سیستم تضمین می‌کند که تنها کسانی که به داده‌های شما دسترسی دارند، می‌توانند به سرویس‌ها دسترسی داشته باشند.\n",
      "\n",
      "از سوی دیگر، فایروال هوشمند که در شبکه NovaConnect استفاده می‌شود، تنها IPهای تأیید‌شده را به شبکه متصل می‌کند. این تضمین می‌کند که تنها کسانی که به شبکه NovaConnect متصل هستند، می‌توانند به داده‌های شما دسترسی داشته باشند.\n",
      "\n",
      "نواستورج همچنین از یک سیستم مدیریت دسترسی و کنترل دسترسی (RBAC) استفاده می‌کند که به شما امکان می‌دهد دسترسی به داده‌های خود را مدیریت کنید و به کسانی که به داده‌های شما دسترسی دارند، محدودیت‌هایی را اعمال کنید.\n",
      "\n",
      "به طور کلی، نواستورج یک سرویس ذخیره‌سازی ابری ایمن و قابل اعتماد است که به شما امکان می‌دهد داده‌های خود را در ابر ذخیره کنید و از امنیت آنها تضمین کنید. اگر به هر گونه سوال یا نگرانی در مورد این سرویس دارید، خوشحالم که می‌توانم به شما کمک کنم.\n",
      "---\n",
      "Processing time: 11.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Follow-up question\n",
    "query2 = \"میشه بیشتر راجع به این سرویس توضیح بدی؟\"\n",
    "print(f\"User query: {query2}\")\n",
    "\n",
    "# Time the response (using existing conversation history)\n",
    "start = time.time()\n",
    "response = rag_chat(query2)\n",
    "end = time.time()\n",
    "\n",
    "# Stream or print the response\n",
    "if stream_option:\n",
    "    stream_text(response)\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: \n",
      "سلام و عرض ادب\n",
      "وقت شما بخیر\n",
      "من یک سوالی داشتم ازتون\n",
      "امکانش هست بفرمایید که در طرح Pro چند هسته پردازشی ارائه می‌شود؟\n",
      "ممنون از شما\n",
      "\n",
      "Similarity scores: [0.7008819108860892]\n",
      "سلام و عرض ادب! وقت شما بخیر. خوشحالم که می‌توانم به سوالتان پاسخ دهم. در طرح حرفه‌ای (Pro)، ۸ هسته پردازشی ارائه می‌شود. این طرح با فضای ذخیره‌سازی ۵۰۰ گیگابایت و ترافیک نامحدود، برای کاربران حرفه‌ای و سازمانی مناسب است. اگر سوالی دیگر دارید، لطفا بپرسید.\n",
      "---\n",
      "Processing time: 2.68 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Different topic question\n",
    "query3 = '''\n",
    "سلام و عرض ادب\n",
    "وقت شما بخیر\n",
    "من یک سوالی داشتم ازتون\n",
    "امکانش هست بفرمایید که در طرح Pro چند هسته پردازشی ارائه می‌شود؟\n",
    "ممنون از شما\n",
    "'''\n",
    "print(f\"User query: {query3}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response = rag_chat(query3)\n",
    "end = time.time()\n",
    "\n",
    "# Stream or print the response\n",
    "if stream_option:\n",
    "    stream_text(response)\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: \n",
      "سلام و عرض ادب\n",
      "وقت شما بخیر\n",
      "من یک سوالی داشتم ازتون\n",
      "امکانش هست بفرمایید که در طرح Pro چند هسته پردازشی ارائه می‌شود؟\n",
      "ممنون از شما\n",
      "\n",
      "Summarized version: سلام و عرض ادب به شما! وقت شما بخیر. خوشحالم که می‌توانم به سوالتان پاسخ دهم.\n",
      "\n",
      "در طرح Pro، 8 هسته پردازشی ارائه می‌شود. این تعداد هسته پردازشی به شما امکان می‌دهد که به راحتی کارهای چندگانه را انجام دهید و از عملکرد بهتر لذت ببرید. علاوه بر این، هسته‌های پردازشی در طرح Pro به شما امکان می‌دهند که از فناوری‌های پیشرفته مانند Turbo Boost و Hyper-Threading استفاده کنید که عملکرد پردازنده را بهبود می‌بخشد.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query3 = '''\n",
    "سلام و عرض ادب\n",
    "وقت شما بخیر\n",
    "من یک سوالی داشتم ازتون\n",
    "امکانش هست بفرمایید که در طرح Pro چند هسته پردازشی ارائه می‌شود؟\n",
    "ممنون از شما\n",
    "'''\n",
    "print(f\"User query: {query3}\")\n",
    "\n",
    "\n",
    "\"\"\"Summarize query using the LLM\"\"\"\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"تو یک دستیار هستی که وظیفه تو خلاصه کردن متن است. سعی نکن به کاربر جواب بدی فقط تشریفات رو از بین ببر و تا جایی که امکان داره خلاصه کن. فقط تا جایی پرامپت کاربر رو کوتاه کن که به هسته اصلی مطلب آسیبی وارد نشه.\"},\n",
    "    {\"role\": \"system\", \"content\": \"خلاصه کن, خلاصه کن, خلاصه کن, خلاصه کن, خلاصه کن\"},\n",
    "    {\"role\": \"user\", \"content\": query3},\n",
    "]\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    top_p=0.85,\n",
    "    temperature=0.1  # Low temperature for more deterministic responses\n",
    ")\n",
    "print(f\"Summarized version: {response['choices'][0]['message']['content']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. RAG System Evaluation\n",
    "\n",
    "Test with more complex queries to evaluate retrieval performance and answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add evaluation functions here if needed\n",
    "def evaluate_retrieval(test_queries, ground_truth_answers):\n",
    "    \"\"\"Simple evaluation for retrieval quality\"\"\"\n",
    "    # TODO: Implement evaluation metrics\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
