{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation with Quantized-Models\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using:\n",
    "- Milvus for vector storage and retrieval\n",
    "- Sentence Transformer for embedding generation\n",
    "- Gemma for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amyrmahdy/GitHub/chatbot/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "from bert_score import score\n",
    "from sentence_transformers import util\n",
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
    "from transformers import logging as transformers_logging\n",
    "from pymilvus import DataType, MilvusClient, WeightedRanker, RRFRanker, AnnSearchRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name_= 'endpoints_info_v5.csv'\n",
    "df = pd.read_csv(data_file_name_)\n",
    "\n",
    "slice_df = df[['title','question','answer','context']]\n",
    "\n",
    "pure_contexts = slice_df['context'].tolist()\n",
    "questions = slice_df['question'].tolist()\n",
    "answers = slice_df['answer'].tolist()\n",
    "titles = slice_df['title'].tolist()\n",
    "qa_as_context = (slice_df['question'] + ' ' + slice_df['answer']).to_list()\n",
    "contexts = []\n",
    "for pure_context in pure_contexts:\n",
    "    contexts.append(pure_context)\n",
    "\n",
    "# for qa in qa_as_context:\n",
    "#     contexts.append(qa)\n",
    "\n",
    "\n",
    "del slice_df\n",
    "del df\n",
    "del pure_contexts\n",
    "del qa_as_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Configure Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_device = \"cuda:0\"\n",
    "# Load embedding model\n",
    "def load_embedding_model(model_name='bge-m3'):\n",
    "    \"\"\"Load and configure the sentence transformer model for embeddings\"\"\"\n",
    "    embedding_model_path = f\"./models/{model_name}\"\n",
    "    \n",
    "    # Load model from local path\n",
    "    embedding_model = BGEM3EmbeddingFunction(\n",
    "        model_name=embedding_model_path,\n",
    "        device=computation_device\n",
    "    )\n",
    "    \n",
    "    return embedding_model\n",
    "\n",
    "# Initialize models\n",
    "embedding_model = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_client_vector_db():\n",
    "    \"\"\"Initialize Milvus and create or get collection\"\"\"\n",
    "    vector_db_client = MilvusClient( \n",
    "    uri = \"http://192.168.100.118:19530\",\n",
    "    user= \"admin\",\n",
    "    password= \"admin\",\n",
    "    db_name= \"default\"\n",
    "    )\n",
    "    return vector_db_client\n",
    "\n",
    "def setup_schema_vector_db(vector_db_client):\n",
    "    schema = vector_db_client.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True\n",
    "        )\n",
    "    # Add fields to schema\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"title\", datatype=DataType.VARCHAR, max_length=2000)\n",
    "    schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=20000)\n",
    "    schema.add_field(field_name=\"sparse\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
    "    schema.add_field(field_name=\"dense\", datatype=DataType.FLOAT_VECTOR, dim=1024)\n",
    "    return schema \n",
    "\n",
    "def setup_index_vector_db(vector_db_client):\n",
    "    index_params = vector_db_client.prepare_index_params()\n",
    "    # Add indexes\n",
    "    index_params.add_index(\n",
    "        field_name=\"dense\",\n",
    "        index_name=\"dense_index\",\n",
    "        index_type=\"AUTOINDEX\",\n",
    "        metric_type=\"IP\"\n",
    "        )\n",
    "\n",
    "    index_params.add_index(\n",
    "        field_name=\"sparse\",\n",
    "        index_name=\"sparse_index\",\n",
    "        index_type=\"SPARSE_INVERTED_INDEX\",  # Index type for sparse vectors\n",
    "        metric_type=\"IP\",  # Currently, only IP (Inner Product) is supported for sparse vectors\n",
    "        params={\"drop_ratio_build\": 0.2},  # The ratio of small vector values to be dropped during indexing\n",
    "        )    \n",
    "    return index_params \n",
    "\n",
    "def setup_collection_vector_db(vector_db_client,collection_name, schema,index_params):\n",
    "    result_drop_collection = vector_db_client.drop_collection(collection_name)\n",
    "    result_setup_collection_vector_db = vector_db_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        index_params=index_params\n",
    "        )    \n",
    "    return result_setup_collection_vector_db \n",
    "\n",
    "\n",
    "def preview_documents_in_collection_vector_db(vector_db_client,collection_name,filter_expression = \"text like '%%'\"):\n",
    "    result_preview_documents_in_collection_vector_db = vector_db_client.query(collection_name, filter = filter_expression, output_fields= [\"title\",\"text\"])\n",
    "    return result_preview_documents_in_collection_vector_db \n",
    "\n",
    "\n",
    "def delete_documents_in_collection_vector_db(vector_db_client,collection_name,filter_expression = \"text like '%%'\"):\n",
    "    result_delete_documents_in_collection_vector_db = vector_db_client.delete(collection_name, filter = filter_expression)\n",
    "    return result_delete_documents_in_collection_vector_db \n",
    "\n",
    "\n",
    "\n",
    "# Set up Milvus\n",
    "collection_name = 'endpoints_info'\n",
    "vector_db_client = setup_client_vector_db()\n",
    "schema_vector_db = setup_schema_vector_db(vector_db_client)\n",
    "index_params_vector_db = setup_index_vector_db(vector_db_client)\n",
    "result_setup_collection_vector_db = setup_collection_vector_db(vector_db_client,collection_name, schema_vector_db,index_params_vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Add Documents to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sparse_vector(encoded_document):    \n",
    "    # Convert to dictionary format {index: value}\n",
    "    coo = encoded_document.tocoo()\n",
    "    sparse_dict = {int(col): float(val) for col, val in zip(coo.col, coo.data)}\n",
    "    return sparse_dict\n",
    "\n",
    "def l2_normalize(vec):\n",
    "    norm = np.linalg.norm(vec)\n",
    "    return (vec / norm).tolist() if norm > 0 else vec\n",
    "\n",
    "def add_documents_to_collection(vector_db_client, collection_name, titles, documents, embedding_model):\n",
    "    \"\"\"Add documents to Milvus collection with embeddings\"\"\"\n",
    "    # Generate embeddings for documents\n",
    "    encoded_documents  = embedding_model.encode_documents(documents)\n",
    "    print(\"Dense vector shape:\", encoded_documents[\"dense\"][0].shape)\n",
    "    print(\"Sparse vector shape:\", list(encoded_documents[\"sparse\"])[0].shape)\n",
    "    \n",
    "    # Prepare id\n",
    "    result_flush = vector_db_client.flush(collection_name)    \n",
    "    row_count = vector_db_client.get_collection_stats(collection_name)['row_count']\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(encoded_documents['dense'])):\n",
    "        # Get the sparse vector for this row\n",
    "        sparse_dict = extract_sparse_vector(encoded_documents['sparse'][i,:])\n",
    "\n",
    "        # Get and normalize dense vector for this row\n",
    "        dense_normalized_list = l2_normalize(encoded_documents['dense'][i].tolist())    \n",
    "\n",
    "        data.append({\n",
    "            \"id\": row_count + i,\n",
    "            \"title\": titles[i],\n",
    "            \"text\": documents[i],\n",
    "            \"sparse\": sparse_dict,\n",
    "            \"dense\": dense_normalized_list \n",
    "        })\n",
    "    \n",
    "    # Add documents with embeddings to collection\n",
    "    result_add_documents_to_collection = vector_db_client.insert(\n",
    "        collection_name=collection_name,\n",
    "        data=data\n",
    "    )\n",
    "    return result_add_documents_to_collection['insert_count']\n",
    "\n",
    "# Add documents to collection\n",
    "insert_count = add_documents_to_collection(vector_db_client, collection_name, titles, contexts, embedding_model)\n",
    "print(f\"Added {insert_count} documents to vector database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_documents_in_collection_vector_db(vector_db_client,collection_name,'id in [0]')\n",
    "# delete_documents_in_collection_vector_db(vector_db_client,collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ann_search_request(query_dense_vector, query_sparse_vector, top_k):\n",
    "    search_param_1 = {\n",
    "        \"data\": [query_dense_vector],\n",
    "        \"anns_field\": \"dense\",\n",
    "        \"param\": {\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"params\": {\"nprobe\": 10}\n",
    "        },\n",
    "        \"limit\": top_k\n",
    "    }\n",
    "    request_1 = AnnSearchRequest(**search_param_1)\n",
    "\n",
    "    search_param_2 = {\n",
    "        \"data\": [query_sparse_vector],\n",
    "        \"anns_field\": \"sparse\",\n",
    "        \"param\": {\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"params\": {\"drop_ratio_build\": 0.2}\n",
    "        },\n",
    "        \"limit\": top_k\n",
    "    }\n",
    "    request_2 = AnnSearchRequest(**search_param_2)\n",
    "\n",
    "    reqs = [request_1, request_2]\n",
    "    \n",
    "    return reqs\n",
    "\n",
    "def retrieve_relevant_documents(vector_db_client, query, collection_name, embedding_model, top_k=2, similarity_threshold=0.48):\n",
    "    \"\"\"Retrieve relevant documents based on semantic similarity\"\"\"\n",
    "    # Create embedding for query\n",
    "    query_embedding = embedding_model.encode_documents([query])\n",
    "    query_dense_vector = l2_normalize(query_embedding['dense'][0].tolist())\n",
    "    query_sparse_vector = extract_sparse_vector(query_embedding['sparse'][0,:])\n",
    "\n",
    "    # Prepare ANN Search Request\n",
    "    requests = prepare_ann_search_request(query_dense_vector, query_sparse_vector, top_k)\n",
    "\n",
    "    # Setup Ranker\n",
    "    ranker = WeightedRanker(0.3, 0.85) \n",
    "\n",
    "    # ranker = RRFRanker(100)\n",
    "\n",
    "    # Query the collection\n",
    "    results = vector_db_client.hybrid_search(\n",
    "        collection_name=collection_name,\n",
    "        reqs=requests,\n",
    "        ranker=ranker,\n",
    "        limit=top_k,\n",
    "        output_fields=[\"text\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    not_found_message = [\"برای پاسخ این سوال با ادمین تماس بگیرید.\"]\n",
    "    not_found_message = [\"\"]\n",
    "\n",
    "    \n",
    "    # Extract results\n",
    "    results = results[0]\n",
    "    documents = []\n",
    "    distances = []\n",
    "    for hits in results:\n",
    "        documents.append(hits['entity']['text'])\n",
    "        distances.append(hits['distance'])\n",
    "    \n",
    "    # Print similarity scores for debugging\n",
    "    print(f\"Similarity scores: {[d for d in distances]}\")\n",
    "    \n",
    "    # Optional: Filter by similarity threshold\n",
    "    filtered_docs = [doc for doc, dist in zip(documents, distances) if dist >= similarity_threshold]\n",
    "    return filtered_docs if filtered_docs else not_found_message\n",
    "    \n",
    "    # return documents\n",
    "\n",
    "# Test retrieval function\n",
    "top_k = 5\n",
    "test_query = questions[14]\n",
    "print(test_query)\n",
    "retrieved_docs = retrieve_relevant_documents(vector_db_client, test_query, collection_name, embedding_model,top_k)\n",
    "# print(retrieved_docs)\n",
    "for retrieved_docs in retrieved_docs:\n",
    "    print(f\"Retrieved document: {retrieved_docs[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Retreival System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(retrieved_contexts, expected_context, top_k=top_k):\n",
    "    \"\"\"\n",
    "    Checks at which rank the expected (gold) context was retrieved\n",
    "    and calculates basic metrics like MRR (Mean reciprocal rank).\n",
    "    \"\"\"\n",
    "    for rank, doc in enumerate(retrieved_contexts, start=1):  # ranks start at 1\n",
    "        if expected_context in doc:\n",
    "            return {\n",
    "                \"rank\": rank,\n",
    "                \"recall@k\": 1 if rank <= top_k else 0,\n",
    "                \"MMR\": 1 / rank\n",
    "            }\n",
    "    \n",
    "    # Not found\n",
    "    return {\n",
    "        \"rank\": None,\n",
    "        \"recall@k\": 0,\n",
    "        \"MMR\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_retrieval_system_report(vector_db_client, questions, expected_contexts, collection_name, top_k):\n",
    "    file_path = 'retrieval_system_report.csv'\n",
    "    \n",
    "    # Check if the file exists to determine if we need to write the header\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    retrieval_system_report = []\n",
    "    with open(file_path, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'question', 'expected_context', 'retrieved_contexts', 'rank', 'recall@k', 'MMR'\n",
    "        ])\n",
    "        \n",
    "        # Write header only if the file doesn't already exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for question, expected_context in zip(questions, expected_contexts):\n",
    "            # Retrieve relevant contexts\n",
    "            retrieved_contexts = retrieve_relevant_documents(vector_db_client, question, collection_name, embedding_model, top_k)\n",
    "            \n",
    "            # Evaluate retrieval metrics\n",
    "            metrics = evaluate_retrieval(retrieved_contexts, expected_context, top_k=top_k)\n",
    "            rank = metrics['rank']\n",
    "            recall_at_k = metrics['recall@k']\n",
    "            mmr = metrics['MMR']\n",
    "            \n",
    "            # Create a dictionary for the current row\n",
    "            row = {\n",
    "                'question': question,\n",
    "                'expected_context': expected_context,\n",
    "                'retrieved_contexts': str(retrieved_contexts),  # Convert list to string\n",
    "                'rank': rank,\n",
    "                'recall@k': recall_at_k,\n",
    "                'MMR': mmr\n",
    "            }\n",
    "\n",
    "            # Write the row to the CSV file immediately\n",
    "            writer.writerow(row)\n",
    "            retrieval_system_report.append(row)\n",
    "\n",
    "            print(\"Question:\", question)\n",
    "            print(\"Found:\", recall_at_k)\n",
    "            print(\"Rank:\", rank)\n",
    "            print(\"MMR:\", mmr)\n",
    "            print(\"Retrieved Contexts count:\", len(retrieved_contexts))\n",
    "            print(\"======================================\")\n",
    "            print(\"======================================\")\n",
    "\n",
    "    print(\"Report saved to the file successfully.\")\n",
    "    return retrieval_system_report\n",
    "\n",
    "# Generate and save the retrieval system report\n",
    "generate_retrieval_system_report(vector_db_client, questions, contexts, collection_name, top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_retrieval_system_report[df_retrieval_system_report['recall@k'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_retrieval_system_report.sort_values(by = ['rank'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Load LLM for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_window_size = 20000\n",
    "\n",
    "def load_llm_model(model_path=\"./models/gemma-3-4b-it.Q2_K/gemma-3-4b-it.Q2_K.Q8_0.gguf\",chat_format='gemma'):\n",
    "    \"\"\"Load and configure the LLM for text generation\"\"\"\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        chat_format=chat_format,\n",
    "        n_gpu_layers=-1,  # Use all available GPU layers\n",
    "        n_ctx=context_window_size,       # Context window size\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "\n",
    "# Define LLM models details like: path and chat_format\n",
    "llm_models_details = {\n",
    "    'dorna-llama3-8b-q8' : {'path': './models/Dorna-Llama3-8B-Instruct-GGUF-Q8/dorna-llama3-8b-instruct.Q8_0.gguf',\n",
    "                            'chat_format': 'llama-3'},\n",
    "    'deepseek-r1-7b-qwen' : {'path': './models/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B.Q8_0.gguf',\n",
    "                            'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-q2': {'path':'./models/gemma-3-4b-it.Q2_K/gemma-3-4b-it.Q2_K.gguf',\n",
    "                     'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-q8': {'path':'./models/gemma-3-4b-it.Q8_0/gemma-3-4b-it.Q8_0.gguf',\n",
    "                      'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-fp16': {'path':'./models/gemma-3-4b-it.fp16/gemma-3-4b-it.fp16.gguf',\n",
    "                        'chat_format': 'gemma'},\n",
    "    'gemma-3-12b-q4': {'path':'./models/gemma-3-12b-it.Q4_0/gemma-3-12b-it-q4_0.gguf',\n",
    "                        'chat_format': 'gemma'}\n",
    "    }\n",
    "\n",
    "# Load Llama model\n",
    "target_llm_model = 'gemma-3-12b-q4'\n",
    "llm_model_path, llm_chat_format = llm_models_details[target_llm_model]['path'], llm_models_details[target_llm_model]['chat_format']\n",
    "llm = load_llm_model(llm_model_path, llm_chat_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "USER_PROMPT_TEMPLATE = '''\n",
    "<role>You are “Technical Assistant.”</role>\n",
    "\n",
    "<goal>\n",
    "Answer technical questions concisely, using **only** chat history or retrieved context, and reply in formal Persian.\n",
    "</goal>\n",
    "\n",
    "<precedence>\n",
    "1.<guardrails> 2.<behavior> 3.Latest user request 4.History 5.Context\n",
    "</precedence>\n",
    "\n",
    "<behavior>\n",
    "  • Pay attention to conversation history always.  \n",
    "  • Follow-ups like «بیشتر توضیح بده / مثال بزن» → extend last answer and explain more details.  \n",
    "  • If no clear keyword remains, ask one clarifying question (Persian).  \n",
    "  • If info is absent in both history & context → output «اطلاعاتی در دسترس نیست. با ادمین تماس بگیرید.»  \n",
    "</behavior>\n",
    "\n",
    "<steps>\n",
    "  0. **Short-Reply**: new question → 2 brief sentence.  \n",
    "  1. **Detailed-Reply**: user asks for more → up to 10 sentences, only existing info.  \n",
    "  2. **Clarify**: ambiguous or keywordless → ask 1 clarifying Q.  \n",
    "  3. **No-Info**: nothing relevant → fixed no-info sentence.\n",
    "</steps>\n",
    "\n",
    "<style>\n",
    "Formal Persian; no emojis; bullets only on request.\n",
    "</style>\n",
    "\n",
    "<guardrails>\n",
    "  • No external knowledge.  \n",
    "  • Reveal technical details only when asked.  \n",
    "  • Reject out-of-scope requests with «درخواست خارج از حیطه است».  \n",
    "  • All answers Persian.  \n",
    "  • If origin queried → «من توسط شرکت وصل ساخته شده‌ام.» (never mention Google).\n",
    "</guardrails>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Conversation history:\n",
    "{history}\n",
    "\n",
    "User:\n",
    "{prompt}\n",
    "'''\n",
    "\n",
    "\n",
    "SUMMARY_PROMPT_TEMPLATE = \"\"\"\n",
    "<role>You are “Conversation Summarizer.”</role>\n",
    "\n",
    "<goal>\n",
    "Return a **concise Persian chronology** of every user–assistant turn in the form:  \n",
    "«اول، کاربر … و دستیار … . دوم، کاربر … و دستیار … . سوم، … .»  \n",
    "Continue sequentially until the last turn; each clause must capture only the key request and the core response.\n",
    "</goal>\n",
    "\n",
    "<precedence>\n",
    "1. <guardrails>  \n",
    "2. <behavior>  \n",
    "3. Conversation history  \n",
    "</precedence>\n",
    "\n",
    "<behavior>\n",
    "  <rule>Process the entire history (summarise internally if > 2 000 characters).</rule>\n",
    "  <rule>Extract just the essential intent of each user message and the essence of the assistant’s reply.</rule>\n",
    "  <rule>Limit the whole summary to ≤ 60 Persian words **or** 6 numbered pairs—whichever comes first.</rule>\n",
    "  <rule>Use the fixed numbered format; no extra commentary, emojis, or citations.</rule>\n",
    "</behavior>\n",
    "\n",
    "<steps>\n",
    "  <step0 title=\"Chronology\">\n",
    "    <trigger>Always (history is present)</trigger>\n",
    "    <action>Return the numbered Persian chronology.</action>\n",
    "  </step0>\n",
    "</steps>\n",
    "\n",
    "<style>\n",
    "  <item>Formal Persian; no bullet points, no emojis.</item>\n",
    "</style>\n",
    "\n",
    "<guardrails>\n",
    "  <rule>No external knowledge or personal assumptions.</rule>\n",
    "  <rule>All outputs must be in Persian.</rule>\n",
    "</guardrails>\n",
    "\n",
    "Conversation History:\n",
    "{conversation_history}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EXTRACT_KEYWORD_PROMPT_TEMPLATE = \"\"\"\n",
    "<role>You are “Keyword Extractor.”</role>\n",
    "\n",
    "<goal>\n",
    "Return the **maximum 10 most relevant Persian keywords / phrases** from the user prompt.  \n",
    "Example – input:  \n",
    "«سلام … کد پاسخ ۵۰۳ معنیش چیه؟ ممنون» → output: «کد پاسخ ۵۰۳»\n",
    "</goal>\n",
    "\n",
    "<precedence>\n",
    "1. <guardrails>  \n",
    "2. <behavior>  \n",
    "3. Latest user prompt\n",
    "</precedence>\n",
    "\n",
    "<behavior>\n",
    "  <rule>Remove greetings, courtesy phrases, punctuation, and stop-words.</rule>\n",
    "  <rule>Otherwise return up to ten Persian words that best capture the technical intent (e.g., «اتصال پایگاه‌داده»).</rule>\n",
    "  <rule>Output must be one Persian phrase, no extra characters, no brackets, no emojis.</rule>\n",
    "  <rule>If no meaningful technical term exists, output the full text without extraction.</rule>\n",
    "</behavior>\n",
    "\n",
    "<steps>\n",
    "  <step0 title=\"Extract\">\n",
    "    <trigger>Always</trigger>\n",
    "    <action>Return the cleaned Persian keywords/phrases per rules.</action>\n",
    "  </step0>\n",
    "</steps>\n",
    "\n",
    "<style>\n",
    "  <item>Output exactly the keywords phrase—nothing else.</item>\n",
    "</style>\n",
    "\n",
    "<guardrails>\n",
    "  <rule>No external knowledge; extract only from the prompt text.</rule>\n",
    "  <rule>All outputs must be in Persian.</rule>\n",
    "</guardrails>\n",
    "\n",
    "User Prompt:\n",
    "{user_prompt}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "def summarize_history_with_llm(history,llm=llm):\n",
    "    \"\"\"Summarize query using the LLM\"\"\"\n",
    "    summary_prompt = SUMMARY_PROMPT_TEMPLATE.format(conversation_history = history)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": summary_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.0  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    # .split('</think>')[-1] if the model thinks!\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    print(f\"Summarized version: {response_clean}\")\n",
    "\n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def extract_keyword_with_llm(user_prompt,llm=llm):\n",
    "    \"\"\"Extract Keyword using the LLM\"\"\"\n",
    "    extract_keyword_prompt = EXTRACT_KEYWORD_PROMPT_TEMPLATE.format(user_prompt = user_prompt)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": extract_keyword_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.0  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    # .split('</think>')[-1] if the model thinks!\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    print(f\"Extracted version: {response_clean}\")\n",
    "\n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def retrieve_context(vector_db_client, query, collection_name=collection_name, embedding_model=embedding_model,top_k=top_k):\n",
    "    \"\"\"Retrieve relevant context based on the query\"\"\"\n",
    "    docs = retrieve_relevant_documents(vector_db_client, query, collection_name, embedding_model,top_k)\n",
    "    for doc in docs:\n",
    "        print(f\"Retrieved document: {doc[:100]}...\")\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "\n",
    "def generate_response_stream(model_input, llm=llm):\n",
    "    \"\"\"Generate streaming response using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{model_input}\"}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1,  # Low temperature for more deterministic responses\n",
    "        repeat_penalty= 1.2,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in response:\n",
    "        delta = chunk['choices'][0]['delta']\n",
    "        if 'content' in delta:\n",
    "            content = delta['content']\n",
    "            print(content, end='', flush=True)\n",
    "            full_response += content\n",
    "            yield content\n",
    "\n",
    "\n",
    "def generate_response(model_input, llm=llm):\n",
    "    \"\"\"Generate response using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": model_input}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1,  # Low temperature for more deterministic responses\n",
    "      repeat_penalty= 1.2\n",
    "    )\n",
    "    # .split('</think>')[-1] if the model thinks!\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    print(response_clean)\n",
    "\n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def rag_chat(user_query, collection_name=collection_name, history=None, stream=False):\n",
    "    \"\"\"Complete RAG pipeline: Retrieve → Generate → Respond\"\"\"\n",
    "    if history is None:\n",
    "        history = conversation_history\n",
    "    \n",
    "    summarized_history_text = \"\"\n",
    "\n",
    "    # Format conversation history\n",
    "    history_text = \"\\n\".join(history)\n",
    "\n",
    "    if history:\n",
    "        summarized_history_text = summarize_history_with_llm(history_text)\n",
    "        \n",
    "    \n",
    "    user_query_keywords = extract_keyword_with_llm(user_query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(vector_db_client, user_query_keywords, collection_name, embedding_model,top_k)\n",
    "\n",
    "        \n",
    "    \n",
    "    # Create prompt with context and history\n",
    "    prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        history=summarized_history_text,\n",
    "        context=context, \n",
    "        prompt=user_query\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        # Generate streaming response\n",
    "        response = \"\"\n",
    "        response_stream = generate_response_stream(prompt)\n",
    "        for chunk in response_stream:\n",
    "            response += chunk\n",
    "    else:\n",
    "        # Generate response (non-stream)\n",
    "        response = generate_response(prompt)\n",
    "\n",
    "\n",
    "    history.append(f\"User: {user_query}\")\n",
    "    history.append(f\"Assistant: {response}\")\n",
    "\n",
    "\n",
    "    \n",
    "    llm.reset()\n",
    "    return response,context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10. Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic question (Without Summary)\n",
    "query1 = questions[1]\n",
    "print(f\"User query: {query1}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query1,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Follow-up question\n",
    "query2 = \"میشه بیشتر راجع به این توضیح بدی؟\"\n",
    "print(f\"User query: {query2}\")\n",
    "\n",
    "# Time the response (using existing conversation history)\n",
    "start = time.time()\n",
    "response, context = rag_chat(query2,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Follow-up question\n",
    "query3 = \"تو رو کی ساخته؟\"\n",
    "print(f\"User query: {query3}\")\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query3,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Follow-up question\n",
    "query4 = \"اولش ازت چی پرسیدم؟\"\n",
    "print(f\"User query: {query4}\")\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query4,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Follow-up question\n",
    "query5 = \"بعدش ازت چی پرسیدم؟\"\n",
    "print(f\"User query: {query5}\")\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query5,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Different topic question\n",
    "query6 = questions[29]\n",
    "print(f\"User query: {query6}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query6,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Follow-up question\n",
    "query7 = \"چرا؟\"\n",
    "print(f\"User query: {query7}\")\n",
    "\n",
    "# Reset conversation history\n",
    "# conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query7,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 11. RAG System Evaluation\n",
    "\n",
    "Test with more complex queries to evaluate retrieval performance and answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_TEMPLATE = \"\"\"\n",
    "<role>You are an Answer-Quality Judge.</role>\n",
    "\n",
    "<goal>\n",
    "  Return ONE and only ONE JSON object with exactly two keys:\n",
    "    \"score\" → an integer 1-10 (no quotes, no decimals)\n",
    "    \"tags\"  → a non-empty JSON array whose elements come only from:\n",
    "              helpful, partially_helpful, unhelpful, off_topic,\n",
    "              unclear, incorrect, incomplete, redundant,\n",
    "              verbose\n",
    "</goal>\n",
    "\n",
    "<rules>\n",
    "  <rule>No other keys, comments, or text are allowed.</rule>\n",
    "  <rule>Do NOT echo the question or answers.</rule>\n",
    "  <rule>Do NOT wrap the JSON in markdown.</rule>\n",
    "  <rule>If you violate any rule, output exactly\n",
    "        {{\\\"error\\\":\\\"invalid output\\\"}} and STOP.</rule>\n",
    "</rules>\n",
    "\n",
    "<output_format>\n",
    "<![CDATA[\n",
    "{{\"score\": <1-10>, \"tags\": [\"<allowed_tag>\", …]}}\n",
    "]]>\n",
    "</output_format>\n",
    "\n",
    "Give me score and tags in JSON format.\n",
    "\n",
    "Question: {question}\n",
    "Answer (Model): {generated_response}\n",
    "Answer (Ground truth): {ground_truth_answer}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_response_llm_as_a_judge(question, generated_response, ground_truth_answer):\n",
    "    \n",
    "    \"\"\"Evaluation for generated response by model vs ground truth answer\"\"\"\n",
    "    judge_prompt = JUDGE_TEMPLATE.format(question = question, generated_response = generated_response, ground_truth_answer = ground_truth_answer)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": judge_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1,  # Low temperature for more deterministic responses\n",
    "        repeat_penalty= 1.2 \n",
    "        )\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    \n",
    "    print(\"LLM as a judge result:\", response_clean)\n",
    "    \n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def evaluate_generated_response_cosine(generated_response, ground_truth_answer,embedding_model=embedding_model):\n",
    "    \n",
    "    \"\"\"Evaluation for generated response by model vs ground truth answer\"\"\"\n",
    "    generated_response_embeddings = embedding_model.encode_documents([generated_response])['dense'][0]\n",
    "    ground_truth_answer_embeddings = embedding_model.encode_documents([ground_truth_answer])['dense'][0]\n",
    "\n",
    "    cosine_score_raw = util.pytorch_cos_sim(generated_response_embeddings, ground_truth_answer_embeddings)\n",
    "    \n",
    "    cosine_score = round(float(cosine_score_raw[0][0]) * 100, 2)\n",
    "    print(\"Cosine Similarity between generated response and ground truth answer:\", cosine_score)\n",
    "    \n",
    "    return cosine_score\n",
    "\n",
    "\n",
    "def evaluate_generated_response_prf(generated_response, ground_truth_answer):\n",
    "    \n",
    "    \"\"\"Evaluation for generated response by model vs ground truth answer\"\"\"\n",
    "\n",
    "    P_raw, R_raw, F1_raw = score([generated_response], [ground_truth_answer], lang='en') # model_type='distilbert-base-uncased'\n",
    "    P = round(float(P_raw[0]) * 100, 2)\n",
    "    R = round(float(R_raw[0]) * 100, 2)\n",
    "    F1 = round(float(F1_raw[0]) * 100, 2)\n",
    "    print(\"Precision: \", P)\n",
    "    print(\"Recall: \", R)\n",
    "    print(\"F1 Score: \", F1)\n",
    "    \n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat_with_processing_time(query):\n",
    "    \"\"\"RAG chat + processing time \"\"\"    \n",
    "    print(f\"User query: {query}\")\n",
    "\n",
    "    # Time the response\n",
    "    start = time.time()\n",
    "    response, context = rag_chat(query,stream=False)\n",
    "    end = time.time()\n",
    "    \n",
    "    processing_time = f\"{end - start:.2f}\"\n",
    "    print(f\"\\nProcessing time: {processing_time} seconds\")\n",
    "    return response, context, processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judgment_json_from_llm_response(llm_response):\n",
    "    judgment_json_str = re.sub(r'^```json\\s*|\\s*```$', '', llm_response).strip()\n",
    "\n",
    "    judgment_json = json.loads(judgment_json_str)\n",
    "    return judgment_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_system_report(questions, answers):\n",
    "    file_path = 'rag_system_report.csv'\n",
    "    \n",
    "    # Check if the file exists to determine if we need to write the header\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    \n",
    "    rag_system_report = [] \n",
    "    with open(file_path, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'question', 'response', 'answer', 'judgement score', 'judgement_tags', \n",
    "            'cosine', 'precision', 'recall', 'f1_score', 'context', 'processing_time'\n",
    "        ])\n",
    "        \n",
    "        # Write header only if the file doesn't already exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        for question, answer in zip(questions, answers):\n",
    "            response, context, processing_time = rag_chat_with_processing_time(question)\n",
    "            llm_judgement = evaluate_generated_response_llm_as_a_judge(question, response, answer)\n",
    "            llm_judgement_json = extract_judgment_json_from_llm_response(llm_judgement)\n",
    "            llm_judgement_judgment_score = llm_judgement_json['score']\n",
    "            llm_judgement_judgment_tags = ', '.join(llm_judgement_json['tags'])\n",
    "            cosine = evaluate_generated_response_cosine(response, answer, embedding_model=embedding_model)\n",
    "            precision, recall, f1_score = evaluate_generated_response_prf(response, answer)\n",
    "\n",
    "            # Create a dictionary to write as a row\n",
    "            row = {\n",
    "                'question': question,\n",
    "                'response': response,\n",
    "                'answer': answer,\n",
    "                'judgement score': llm_judgement_judgment_score,\n",
    "                'judgement_tags': llm_judgement_judgment_tags,\n",
    "                'cosine': cosine,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1_score,\n",
    "                'context': context,\n",
    "                'processing_time': processing_time\n",
    "            }\n",
    "\n",
    "            # Write the row to the CSV file immediately\n",
    "            writer.writerow(row)\n",
    "            rag_system_report.append(row)\n",
    "            print(\"======================================\")\n",
    "            print(\"======================================\")\n",
    "\n",
    "    print(\"Report saved to the file successfully.\")\n",
    "    return rag_system_report\n",
    "\n",
    "# Generate and save the report\n",
    "generate_rag_system_report(questions, answers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
