{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f2225fe1ba470db560cac41bb8a7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import chromadb\n",
    "from langchain import PromptTemplate\n",
    "from transformers import AutoTokenizer, pipeline,AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Local path to the downloaded model files\n",
    "lllm_model_path = \"./models/falcon-7b-instruct\"\n",
    "# offload_folder_path = \"./offload\"\n",
    "# Load tokenizer and model from the local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(lllm_model_path)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(lllm_model_path, device_map=\"cpu\") # , offload_folder=offload_folder_path\n",
    "llm_model.config.pad_token_id = llm_model.config.eos_token_id\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", #task\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cpu\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "You are an intelligent chatbot. Help the following question with brilliant answers.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "question = \"Elon musk VS Donald trump for president election\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an intelligent chatbot. Help the following question with brilliant answers.\n",
      "Question: Elon musk VS Donald trump for president election\n",
      "Answer:\n",
      "Elon Musk is an intelligent and highly successful entrepreneur. He is a pioneer in several fields, including electric cars and space exploration. On the other hand, Donald Trump has been a successful businessman but is not a professional inventor. In terms of presidential qualifications, both have been successful in their respective fields, which means they both have unique skillsets. Ultimately, it is up to the voters to decide which skillset is more important for the presidency.\n"
     ]
    }
   ],
   "source": [
    "result = llm_chain.invoke({\"question\" : question })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk is an intelligent and highly successful entrepreneur. He is a pioneer in several fields, including electric cars and space exploration. On the other hand, Donald Trump has been a successful businessman but is not a professional inventor. In terms of presidential qualifications, both have been successful in their respective fields, which means they both have unique skillsets. Ultimately, it is up to the voters to decide which skillset is more important for the presidency.\n"
     ]
    }
   ],
   "source": [
    "if \"Answer:\" in result:\n",
    "    clean_result = result.split(\"Answer:\")[-1].strip()\n",
    "else:\n",
    "    clean_result = result.strip()\n",
    "\n",
    "print(clean_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Required Files: Download the following files (usually found in the \"Files and versions\" section):\n",
    "\n",
    "# config.json\n",
    "# pytorch_model.bin (or multiple weight shards, like pytorch_model-00001-of-00002.bin, etc.)\n",
    "# tokenizer.json\n",
    "# special_tokens_map.json\n",
    "# tokenizer_config.json\n",
    "# configuration_falcon.py\n",
    "# modeling_falcon.py\n",
    "# pytorch_model.bin.index.json\n",
    "# generation_config.json (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Sentence-Transformers model\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight model for embeddings\n",
    "\n",
    "# Save model to use it localy later\n",
    "# embedding_model_path = \"./models/all-MiniLM-L6-v2\"\n",
    "# embedding_model.save(embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from path\n",
    "embedding_model_path = \"./models/all-MiniLM-L6-v2\"\n",
    "embedding_model = SentenceTransformer(embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"./chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the collection\n",
    "client.delete_collection(\"hello-world\")\n",
    "# Create or load a collection\n",
    "collection = client.get_or_create_collection(\"hello-world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents, Embeddings and metadata\n",
    "texts = [\"ChromaDB is open-source.\", \"ChromaDB is a vector database.\", \"Sentence-Transformers generate embeddings.\"]\n",
    "embeddings_texts = embedding_model.encode(texts)\n",
    "ids = [\"doc1\",\"doc2\",\"doc3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to ChromaDB\n",
    "collection.add(\n",
    "    documents = texts,\n",
    "    embeddings = embeddings_texts,\n",
    "    ids = ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is ChromaDB?\"\n",
    "query_embedding = embedding_model.encode(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings = query_embedding,\n",
    "    n_results = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_summary(customer_query,retrieved_document_1,retrieved_document_2):\n",
    "    return f'''System Query: You are a Support Team Member. Help the customers.\n",
    "    Customer Query: {customer_query}\n",
    "    Retrieved Documents for context-awareness:\n",
    "    {retrieved_document_1}\n",
    "    {retrieved_document_2}\n",
    "    Summarize the retrieved documents and answer:\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = template_summary(query_text,results['documents'][0][0],results['documents'][0][1])\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\") #.to(\"cuda\")  # Move to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = llm_model.generate(inputs[\"input_ids\"], max_length=100, temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Summarize the retrieved documents and answer:\" in raw_summary:\n",
    "    summary = raw_summary.split(\"Summarize the retrieved documents and answer:\")[-1].strip()\n",
    "else:\n",
    "    summary = raw_summary.strip()\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbotvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
