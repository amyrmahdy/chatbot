{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Required Files: Download the following files (usually found in the \"Files and versions\" section):\n",
    "\n",
    "# config.json\n",
    "# pytorch_model.bin (or multiple weight shards, like pytorch_model-00001-of-00002.bin, etc.)\n",
    "# tokenizer.json\n",
    "# special_tokens_map.json\n",
    "# tokenizer_config.json\n",
    "# configuration_falcon.py\n",
    "# modeling_falcon.py\n",
    "# pytorch_model.bin.index.json\n",
    "# generation_config.json (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099092abd1f74f35881d06302db0da12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Local path to the downloaded model files\n",
    "lllm_model_path = \"./models/falcon-7b-instruct\"\n",
    "# offload_folder_path = \"./offload\"\n",
    "# Load tokenizer and model from the local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(lllm_model_path)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(lllm_model_path, device_map=\"cpu\") # , offload_folder=offload_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Sentence-Transformers model\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight model for embeddings\n",
    "\n",
    "# Save model to use it localy later\n",
    "# embedding_model_path = \"./models/all-MiniLM-L6-v2\"\n",
    "# embedding_model.save(embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from path\n",
    "embedding_model_path = \"./models/all-MiniLM-L6-v2\"\n",
    "embedding_model = SentenceTransformer(embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"./chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the collection\n",
    "client.delete_collection(\"hello-world\")\n",
    "# Create or load a collection\n",
    "collection = client.get_or_create_collection(\"hello-world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents, Embeddings and metadata\n",
    "texts = [\"ChromaDB is open-source.\", \"ChromaDB is a vector database.\", \"Sentence-Transformers generate embeddings.\"]\n",
    "embeddings_texts = embedding_model.encode(texts)\n",
    "ids = [\"doc1\",\"doc2\",\"doc3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to ChromaDB\n",
    "collection.add(\n",
    "    documents = texts,\n",
    "    embeddings = embeddings_texts,\n",
    "    ids = ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is ChromaDB?\"\n",
    "query_embedding = embedding_model.encode(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings = query_embedding,\n",
    "    n_results = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc2', 'doc1']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['ChromaDB is a vector database.', 'ChromaDB is open-source.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None]],\n",
       " 'distances': [[0.525861771396575, 0.6587712137600167]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_summary(customer_query,retrieved_document_1,retrieved_document_2):\n",
    "    return f'''System Query: You are a Support Team Member. Help the customers.\n",
    "    Customer Query: {customer_query}\n",
    "    Retrieved Documents for context-awareness:\n",
    "    {retrieved_document_1}\n",
    "    {retrieved_document_2}\n",
    "    Summarize the retrieved documents and answer:\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System Query: You are a Support Team Member. Help the customers.\\n    Customer Query: What is ChromaDB?\\n    Retrieved Documents for context-awareness:\\n    ChromaDB is a vector database.\\n    ChromaDB is open-source.\\n    Summarize the retrieved documents and answer:\\n    '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = template_summary(query_text,results['documents'][0][0],results['documents'][0][1])\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\") #.to(\"cuda\")  # Move to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[10721, 29213,    37,   781,   362,   241,  6624,  5888, 11103,    25,\n",
       "          8032,   248,  2536,    25,   742,  9455, 29213,    37,  1634,   304,\n",
       "         26433,    76,  9814,    42,   742, 18934, 30499,   312,  4436,    24,\n",
       "         49606,    37,   742, 26433,    76,  9814,   304,   241, 12586,  6729,\n",
       "            25,   742, 26433,    76,  9814,   304,  1314,    24,  8679,    25,\n",
       "           742, 12753,   270,   907,   248, 34797,  5759,   273,  3173,    37,\n",
       "           561]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasl/EXTERA_VOLUME_1/vasl_chatbot/chatbot/venv/lib64/python3.12/site-packages/transformers/generation/configuration_utils.py:598: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output_ids = llm_model.generate(inputs[\"input_ids\"], max_length=100, temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10721, 29213,    37,   781,   362,   241,  6624,  5888, 11103,    25,\n",
       "          8032,   248,  2536,    25,   742,  9455, 29213,    37,  1634,   304,\n",
       "         26433,    76,  9814,    42,   742, 18934, 30499,   312,  4436,    24,\n",
       "         49606,    37,   742, 26433,    76,  9814,   304,   241, 12586,  6729,\n",
       "            25,   742, 26433,    76,  9814,   304,  1314,    24,  8679,    25,\n",
       "           742, 12753,   270,   907,   248, 34797,  5759,   273,  3173,    37,\n",
       "           561,    39,    91,    41,  1620,   397,    76,  9814,   304,   241,\n",
       "         12586,  6729,   325,   304,  1314,    24,  8679,   273,   418,   314,\n",
       "          1042,   312,  1211,  3549,   273, 29351,    25,   605,   304,   241,\n",
       "          4452,  2119,   325,   418,   314,  1042,   271, 39147,   273, 13203]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>ChromaDB is a vector database that is open-source and can be used for data analysis and visualization. It is a powerful tool that can be used to summarize and analyze\n"
     ]
    }
   ],
   "source": [
    "if \"Summarize the retrieved documents and answer:\" in raw_summary:\n",
    "    summary = raw_summary.split(\"Summarize the retrieved documents and answer:\")[-1].strip()\n",
    "else:\n",
    "    summary = raw_summary.strip()\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbotvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
