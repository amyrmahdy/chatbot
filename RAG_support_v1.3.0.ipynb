{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation with Quantized-Models\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using:\n",
    "- Milvus for vector storage and retrieval\n",
    "- Sentence Transformer for embedding generation\n",
    "- Gemma for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amyrmahdy/GitHub/chatbot/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "from bert_score import score\n",
    "from sentence_transformers import util\n",
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
    "from transformers import logging as transformers_logging\n",
    "from pymilvus import DataType, MilvusClient, WeightedRanker, RRFRanker, AnnSearchRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name_= 'endpoints_info_v5.csv'\n",
    "df = pd.read_csv(data_file_name_)\n",
    "\n",
    "slice_df = df[['title','question','answer','context']]\n",
    "\n",
    "pure_contexts = slice_df['context'].tolist()\n",
    "questions = slice_df['question'].tolist()\n",
    "answers = slice_df['answer'].tolist()\n",
    "titles = slice_df['title'].tolist()\n",
    "qa_as_context = (slice_df['question'] + ' ' + slice_df['answer']).to_list()\n",
    "contexts = []\n",
    "for pure_context in pure_contexts:\n",
    "    contexts.append(pure_context)\n",
    "\n",
    "# for qa in qa_as_context:\n",
    "#     contexts.append(qa)\n",
    "\n",
    "\n",
    "del slice_df\n",
    "del df\n",
    "del pure_contexts\n",
    "del qa_as_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Configure Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_device = \"cuda:0\"\n",
    "# Load embedding model\n",
    "def load_embedding_model(model_name='bge-m3'):\n",
    "    \"\"\"Load and configure the sentence transformer model for embeddings\"\"\"\n",
    "    embedding_model_path = f\"./models/{model_name}\"\n",
    "    \n",
    "    # Load model from local path\n",
    "    embedding_model = BGEM3EmbeddingFunction(\n",
    "        model_name=embedding_model_path,\n",
    "        device=computation_device\n",
    "    )\n",
    "    \n",
    "    return embedding_model\n",
    "\n",
    "# Initialize models\n",
    "embedding_model = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_client_vector_db():\n",
    "    \"\"\"Initialize Milvus and create or get collection\"\"\"\n",
    "    vector_db_client = MilvusClient( \n",
    "    uri = \"http://192.168.100.118:19530\",\n",
    "    user= \"admin\",\n",
    "    password= \"admin\",\n",
    "    db_name= \"default\"\n",
    "    )\n",
    "    return vector_db_client\n",
    "\n",
    "def setup_schema_vector_db(vector_db_client):\n",
    "    schema = vector_db_client.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True\n",
    "        )\n",
    "    # Add fields to schema\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"title\", datatype=DataType.VARCHAR, max_length=2000)\n",
    "    schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=20000)\n",
    "    schema.add_field(field_name=\"sparse\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
    "    schema.add_field(field_name=\"dense\", datatype=DataType.FLOAT_VECTOR, dim=1024)\n",
    "    return schema \n",
    "\n",
    "def setup_index_vector_db(vector_db_client):\n",
    "    index_params = vector_db_client.prepare_index_params()\n",
    "    # Add indexes\n",
    "    index_params.add_index(\n",
    "        field_name=\"dense\",\n",
    "        index_name=\"dense_index\",\n",
    "        index_type=\"AUTOINDEX\",\n",
    "        metric_type=\"IP\"\n",
    "        )\n",
    "\n",
    "    index_params.add_index(\n",
    "        field_name=\"sparse\",\n",
    "        index_name=\"sparse_index\",\n",
    "        index_type=\"SPARSE_INVERTED_INDEX\",  # Index type for sparse vectors\n",
    "        metric_type=\"IP\",  # Currently, only IP (Inner Product) is supported for sparse vectors\n",
    "        params={\"drop_ratio_build\": 0.2},  # The ratio of small vector values to be dropped during indexing\n",
    "        )    \n",
    "    return index_params \n",
    "\n",
    "def setup_collection_vector_db(vector_db_client,collection_name, schema,index_params):\n",
    "    result_drop_collection = vector_db_client.drop_collection(collection_name)\n",
    "    result_setup_collection_vector_db = vector_db_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        index_params=index_params\n",
    "        )    \n",
    "    return result_setup_collection_vector_db \n",
    "\n",
    "\n",
    "def preview_documents_in_collection_vector_db(vector_db_client,collection_name,filter_expression = \"text like '%%'\"):\n",
    "    result_preview_documents_in_collection_vector_db = vector_db_client.query(collection_name, filter = filter_expression, output_fields= [\"title\",\"text\"])\n",
    "    return result_preview_documents_in_collection_vector_db \n",
    "\n",
    "\n",
    "def delete_documents_in_collection_vector_db(vector_db_client,collection_name,filter_expression = \"text like '%%'\"):\n",
    "    result_delete_documents_in_collection_vector_db = vector_db_client.delete(collection_name, filter = filter_expression)\n",
    "    return result_delete_documents_in_collection_vector_db \n",
    "\n",
    "\n",
    "\n",
    "# Set up Milvus\n",
    "collection_name = 'endpoints_info'\n",
    "vector_db_client = setup_client_vector_db()\n",
    "schema_vector_db = setup_schema_vector_db(vector_db_client)\n",
    "index_params_vector_db = setup_index_vector_db(vector_db_client)\n",
    "result_setup_collection_vector_db = setup_collection_vector_db(vector_db_client,collection_name, schema_vector_db,index_params_vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Add Documents to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector shape: (1024,)\n",
      "Sparse vector shape: (250002,)\n",
      "Added 134 documents to vector database\n"
     ]
    }
   ],
   "source": [
    "def extract_sparse_vector(encoded_document):    \n",
    "    # Convert to dictionary format {index: value}\n",
    "    coo = encoded_document.tocoo()\n",
    "    sparse_dict = {int(col): float(val) for col, val in zip(coo.col, coo.data)}\n",
    "    return sparse_dict\n",
    "\n",
    "def l2_normalize(vec):\n",
    "    norm = np.linalg.norm(vec)\n",
    "    return (vec / norm).tolist() if norm > 0 else vec\n",
    "\n",
    "def add_documents_to_collection(vector_db_client, collection_name, titles, documents, embedding_model):\n",
    "    \"\"\"Add documents to Milvus collection with embeddings\"\"\"\n",
    "    # Generate embeddings for documents\n",
    "    encoded_documents  = embedding_model.encode_documents(documents)\n",
    "    print(\"Dense vector shape:\", encoded_documents[\"dense\"][0].shape)\n",
    "    print(\"Sparse vector shape:\", list(encoded_documents[\"sparse\"])[0].shape)\n",
    "    \n",
    "    # Prepare id\n",
    "    result_flush = vector_db_client.flush(collection_name)    \n",
    "    row_count = vector_db_client.get_collection_stats(collection_name)['row_count']\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(encoded_documents['dense'])):\n",
    "        # Get the sparse vector for this row\n",
    "        sparse_dict = extract_sparse_vector(encoded_documents['sparse'][i,:])\n",
    "\n",
    "        # Get and normalize dense vector for this row\n",
    "        dense_normalized_list = l2_normalize(encoded_documents['dense'][i].tolist())    \n",
    "\n",
    "        data.append({\n",
    "            \"id\": row_count + i,\n",
    "            \"title\": titles[i],\n",
    "            \"text\": documents[i],\n",
    "            \"sparse\": sparse_dict,\n",
    "            \"dense\": dense_normalized_list \n",
    "        })\n",
    "    \n",
    "    # Add documents with embeddings to collection\n",
    "    result_add_documents_to_collection = vector_db_client.insert(\n",
    "        collection_name=collection_name,\n",
    "        data=data\n",
    "    )\n",
    "    return result_add_documents_to_collection['insert_count']\n",
    "\n",
    "# Add documents to collection\n",
    "insert_count = add_documents_to_collection(vector_db_client, collection_name, titles, contexts, embedding_model)\n",
    "print(f\"Added {insert_count} documents to vector database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_documents_in_collection_vector_db(vector_db_client,collection_name,'id in [0]')\n",
    "# delete_documents_in_collection_vector_db(vector_db_client,collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سرویسای دیگه هم   به افرا  اضافه میشه یا فقط همیناست؟\n",
      "Similarity scores: []\n",
      "Retrieved document: ...\n"
     ]
    }
   ],
   "source": [
    "def prepare_ann_search_request(query_dense_vector, query_sparse_vector, top_k):\n",
    "    search_param_1 = {\n",
    "        \"data\": [query_dense_vector],\n",
    "        \"anns_field\": \"dense\",\n",
    "        \"param\": {\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"params\": {\"nprobe\": 10}\n",
    "        },\n",
    "        \"limit\": top_k\n",
    "    }\n",
    "    request_1 = AnnSearchRequest(**search_param_1)\n",
    "\n",
    "    search_param_2 = {\n",
    "        \"data\": [query_sparse_vector],\n",
    "        \"anns_field\": \"sparse\",\n",
    "        \"param\": {\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"params\": {\"drop_ratio_build\": 0.2}\n",
    "        },\n",
    "        \"limit\": top_k\n",
    "    }\n",
    "    request_2 = AnnSearchRequest(**search_param_2)\n",
    "\n",
    "    reqs = [request_1, request_2]\n",
    "    \n",
    "    return reqs\n",
    "\n",
    "def retrieve_relevant_documents(vector_db_client, query, collection_name, embedding_model, top_k=2, similarity_threshold=0.48):\n",
    "    \"\"\"Retrieve relevant documents based on semantic similarity\"\"\"\n",
    "    # Create embedding for query\n",
    "    query_embedding = embedding_model.encode_documents([query])\n",
    "    query_dense_vector = l2_normalize(query_embedding['dense'][0].tolist())\n",
    "    query_sparse_vector = extract_sparse_vector(query_embedding['sparse'][0,:])\n",
    "\n",
    "    # Prepare ANN Search Request\n",
    "    requests = prepare_ann_search_request(query_dense_vector, query_sparse_vector, top_k)\n",
    "\n",
    "    # Setup Ranker\n",
    "    ranker = WeightedRanker(0.3, 0.85) \n",
    "\n",
    "    # ranker = RRFRanker(100)\n",
    "\n",
    "    # Query the collection\n",
    "    results = vector_db_client.hybrid_search(\n",
    "        collection_name=collection_name,\n",
    "        reqs=requests,\n",
    "        ranker=ranker,\n",
    "        limit=top_k,\n",
    "        output_fields=[\"text\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    not_found_message = [\"برای پاسخ این سوال با ادمین تماس بگیرید.\"]\n",
    "    not_found_message = [\"\"]\n",
    "\n",
    "    \n",
    "    # Extract results\n",
    "    results = results[0]\n",
    "    documents = []\n",
    "    distances = []\n",
    "    for hits in results:\n",
    "        documents.append(hits['entity']['text'])\n",
    "        distances.append(hits['distance'])\n",
    "    \n",
    "    # Print similarity scores for debugging\n",
    "    print(f\"Similarity scores: {[d for d in distances]}\")\n",
    "    \n",
    "    # Optional: Filter by similarity threshold\n",
    "    filtered_docs = [doc for doc, dist in zip(documents, distances) if dist >= similarity_threshold]\n",
    "    return filtered_docs if filtered_docs else not_found_message\n",
    "    \n",
    "    # return documents\n",
    "\n",
    "# Test retrieval function\n",
    "top_k = 5\n",
    "test_query = questions[14]\n",
    "print(test_query)\n",
    "retrieved_docs = retrieve_relevant_documents(vector_db_client, test_query, collection_name, embedding_model,top_k)\n",
    "# print(retrieved_docs)\n",
    "for retrieved_docs in retrieved_docs:\n",
    "    print(f\"Retrieved document: {retrieved_docs[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Retreival System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(retrieved_contexts, expected_context, top_k=top_k):\n",
    "    \"\"\"\n",
    "    Checks at which rank the expected (gold) context was retrieved\n",
    "    and calculates basic metrics like MRR (Mean reciprocal rank).\n",
    "    \"\"\"\n",
    "    for rank, doc in enumerate(retrieved_contexts, start=1):  # ranks start at 1\n",
    "        if expected_context in doc:\n",
    "            return {\n",
    "                \"rank\": rank,\n",
    "                \"recall@k\": 1 if rank <= top_k else 0,\n",
    "                \"MMR\": 1 / rank\n",
    "            }\n",
    "    \n",
    "    # Not found\n",
    "    return {\n",
    "        \"rank\": None,\n",
    "        \"recall@k\": 0,\n",
    "        \"MMR\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores: []\n",
      "Question:  x trace id برای چیه؟\n",
      "Found:  0\n",
      "Rank:  None\n",
      "MMR:  0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: []\n",
      "Question:  x-trace-id کجاها استفاده میشه؟\n",
      "Found:  0\n",
      "Rank:  None\n",
      "MMR:  0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6897344589233398, 0.6897344589233398, 0.6897344589233398, 0.6814343929290771, 0.6811771988868713]\n",
      "Question:  باندل آی دی برای چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7057149410247803, 0.7057149410247803, 0.7057149410247803, 0.6708484888076782, 0.669792890548706]\n",
      "Question:  bundle id برای چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7499792575836182, 0.7202954888343811, 0.6801072359085083, 0.6636160612106323, 0.44051843881607056]\n",
      "Question:  کلید انحصاری اپلیکیشن user app key چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7084743976593018, 0.7080954909324646, 0.6679520606994629, 0.6679520606994629, 0.6679520606994629]\n",
      "Question:  باندل آی دی و کلید انحصاریو چجوری میتونم بگیرم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6933762431144714, 0.6803498864173889, 0.6803498864173889, 0.6658591032028198, 0.6572594046592712]\n",
      "Question:  پنل مشتریان برای چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6790294647216797, 0.6439365744590759, 0.6374342441558838, 0.6374342441558838, 0.4342224597930908]\n",
      "Question:  بخش مدیریت رایا کاربردش چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6732504367828369, 0.6732504367828369, 0.6732504367828369, 0.4351417124271393, 0.4342072606086731]\n",
      "Question:  برای تمدید بسته باید چیکار کنم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7105655670166016, 0.7105655670166016, 0.6730865240097046, 0.6623926162719727, 0.4532524645328522]\n",
      "Question:  نام کاربری و کلمه عبور پنل مشتریانو از کجا باید بگیرم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6781128644943237, 0.6711101531982422, 0.6711101531982422, 0.6610931158065796, 0.44984170794487]\n",
      "Question:  اگه درباره پنل مشتریان سوال داشتم، چیکار کنم؟\n",
      "Found:  1\n",
      "Rank:  2\n",
      "MMR:  0.5\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7264406681060791, 0.6887895464897156, 0.6826438307762146, 0.6826438307762146, 0.6826438307762146]\n",
      "Question:  برای تغییر bundle-id و user-key چیکار کنم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6610751152038574, 0.6458283066749573, 0.4504178762435913, 0.44945526123046875, 0.4474087357521057]\n",
      "Question:  میخوام سرویسامو غیر فعال کنم باید چیکار کنم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7073289155960083, 0.6910287141799927, 0.6871670484542847, 0.47879889607429504, 0.4786921441555023]\n",
      "Question:  سرویسای ارائه شده توسط افرا چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6897003650665283, 0.6841670274734497, 0.6794514656066895, 0.4794250428676605, 0.4790061116218567]\n",
      "Question:  سرویسای دیگه هم   به افرا  اضافه میشه یا فقط همیناست؟\n",
      "Found:  0\n",
      "Rank:  None\n",
      "MMR:  0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6764219403266907, 0.6557677984237671, 0.4439345598220825, 0.44347020983695984, 0.44347020983695984]\n",
      "Question:  راه‌های ارتباط با پشتیبانی به چه صورته؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6797553300857544, 0.6516050696372986, 0.45370444655418396, 0.44997045397758484, 0.44846445322036743]\n",
      "Question:  اگر در  پیاده سازی سرویس‌ها مشکلی پیش بیاد از کجا میتونم سوال کنم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.698493242263794, 0.6385267376899719, 0.43632087111473083, 0.4339056611061096, 0.4339056611061096]\n",
      "Question:  تسویه قبض‌های خلافی خودرو اکی نمیشه چیکار کنم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.710270881652832, 0.710270881652832, 0.6877387762069702, 0.4703688323497772, 0.47008687257766724]\n",
      "Question:  آدرس اپراتور افرا کجاست؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7000076174736023, 0.7000076174736023, 0.6728713512420654, 0.4659702181816101, 0.46505969762802124]\n",
      "Question:  ساعت کاری افرا به چه صورته؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6687976121902466, 0.6597648859024048, 0.4320770502090454, 0.43146225810050964, 0.43037351965904236]\n",
      "Question:  صدور صورتحساب به چه صورته؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6471580862998962, 0.6457191705703735, 0.645719051361084, 0.645719051361084, 0.6398942470550537]\n",
      "Question:  چقدر وقت دارم صورت حسابو تایید کنم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6676187515258789, 0.6676187515258789, 0.6676187515258789, 0.6491448879241943, 0.441312700510025]\n",
      "Question:  تسویه فاکتورهای ماهانه به چه صورته؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6587311625480652, 0.6587311029434204, 0.6587311029434204, 0.6330975890159607, 0.4355767071247101]\n",
      "Question:  شماره حساب شرکتو از کجا بگیرم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6646096110343933, 0.6590603590011597, 0.4406715929508209, 0.4376007318496704, 0.43758919835090637]\n",
      "Question:  چقدر وقت دارم صورتحسابو پرداخت کنم؟\n",
      "Found:  0\n",
      "Rank:  None\n",
      "MMR:  0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6752112507820129, 0.6598862409591675, 0.4488258361816406, 0.4483204782009125, 0.4474028944969177]\n",
      "Question:  در صورت وجود اختلال در سرویس‌ها اطلاع رسانی به چه صورته؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6673213839530945, 0.6486310958862305, 0.6477245688438416, 0.44224977493286133, 0.4422468841075897]\n",
      "Question:  در صورت وجود مغایرت در صورت وضعیت ماهانه چه اقدامی باید انجام بدم؟\n",
      "Found:  1\n",
      "Rank:  2\n",
      "MMR:  0.5\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6477404236793518, 0.44071200489997864, 0.4389438331127167, 0.4387109577655792, 0.4387109577655792]\n",
      "Question:  برای چی باید آدرس ip اعلام کنم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6714961528778076, 0.6714961528778076, 0.6520641446113586, 0.4567694365978241, 0.454994797706604]\n",
      "Question:  چجوری میتونم برای مشتریم دسترسی بگیرم؟\n",
      "Found:  0\n",
      "Rank:  None\n",
      "MMR:  0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.634101390838623, 0.634101390838623, 0.45197704434394836, 0.4427489638328552, 0.43891188502311707]\n",
      "Question:  آیا مجوز نگهداری موقت و یا دائم این داده‌هارو دارم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6362285614013672, 0.6362285614013672, 0.43571367859840393, 0.4345674514770508, 0.4341880679130554]\n",
      "Question:  اگر داده‌هارو نگه دارم چه اتفاقی رخ میده؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6946213245391846, 0.6718695163726807, 0.46166810393333435, 0.46088680624961853, 0.45512980222702026]\n",
      "Question:  متد فراخوانی سرویس‎ها چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7098391056060791, 0.7097275853157043, 0.6579526662826538, 0.47465580701828003, 0.4590342938899994]\n",
      "Question:  مرچنت آیدی و مرچنت کد در سرویس استعلام وضعیت وسیله نقلیه چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7060912847518921, 0.7060887813568115, 0.6591067910194397, 0.47501683235168457, 0.4596974849700928]\n",
      "Question:  چطوری مرچنت آیدی و مرچنت کد برای سرویس استعلام وضعیت وسیله نقلیه بگیرم؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6580045223236084, 0.6512690782546997, 0.45139533281326294, 0.4511169195175171, 0.4505287706851959]\n",
      "Question:  جزئیات بیشتر ورودی و خروجی سرویس‌ها را چجوری تهیه کنم؟\n",
      "Found:  1\n",
      "Rank:  2\n",
      "MMR:  0.5\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6882444620132446, 0.6309697031974792, 0.6296411752700806, 0.435043066740036, 0.43487924337387085]\n",
      "Question:  چه تراکنش‌هایی منجر به محاسبه تعرفه کامل خدمت میشه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6995303630828857, 0.6698508262634277, 0.4709897041320801, 0.4650396406650543, 0.46500101685523987]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 200 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6756651401519775, 0.6723881959915161, 0.6699897646903992, 0.4726903438568115, 0.4663255214691162]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 202 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  0\n",
      "Rank:  None\n",
      "MMR:  0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.695846438407898, 0.6710120439529419, 0.47193610668182373, 0.4684191048145294, 0.4659177362918854]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 400 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6748291850090027, 0.4736313223838806, 0.4707730710506439, 0.4675285518169403, 0.4658162593841553]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 401 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  0\n",
      "Rank:  None\n",
      "MMR:  0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6951805353164673, 0.6831356883049011, 0.673119068145752, 0.4697488844394684, 0.46961361169815063]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 403 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6923708915710449, 0.6790606379508972, 0.4736897945404053, 0.4705162048339844, 0.4676503539085388]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 404 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.691335916519165, 0.6744309067726135, 0.4742075204849243, 0.47057992219924927, 0.4673091769218445]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 408 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6978439092636108, 0.4727117121219635, 0.46870455145835876, 0.46730780601501465, 0.46617206931114197]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 410 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7040749788284302, 0.6820824146270752, 0.6728262901306152, 0.47085705399513245, 0.46762844920158386]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 429 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7028422355651855, 0.6815908551216125, 0.6743109226226807, 0.6734342575073242, 0.4670889377593994]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 499 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6919797658920288, 0.6752904653549194, 0.46766722202301025, 0.4653848111629486, 0.46528926491737366]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 500 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.694881021976471, 0.687611997127533, 0.682227611541748, 0.6737073659896851, 0.4688361883163452]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 503 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6944721937179565, 0.6826000213623047, 0.6793303489685059, 0.4704607129096985, 0.4690094590187073]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 504 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6925098299980164, 0.4711865782737732, 0.46793046593666077, 0.46557489037513733, 0.4655388593673706]\n",
      "Question:  سرویسو فراخوانی کردم، کد وضعیت 520 گرفتم، این کد وضعیت چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6945748925209045, 0.6825124025344849, 0.47056058049201965, 0.4662415683269501, 0.4658876359462738]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 200 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6989277005195618, 0.4728623330593109, 0.4685055911540985, 0.4681190252304077, 0.4635376036167145]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 202 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7111213207244873, 0.6743634939193726, 0.4730130136013031, 0.46806949377059937, 0.46294787526130676]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 302 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6924130916595459, 0.6716493368148804, 0.6711534857749939, 0.46886494755744934, 0.4645158648490906]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 311 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7111245393753052, 0.6746434569358826, 0.4679771363735199, 0.4629918932914734, 0.46267956495285034]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 330 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6922083497047424, 0.6847567558288574, 0.471773624420166, 0.4672006070613861, 0.4668898284435272]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 400 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6960850358009338, 0.6808947324752808, 0.4710129499435425, 0.47058144211769104, 0.46998175978660583]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 401 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7038887739181519, 0.6782659888267517, 0.47201845049858093, 0.4706658720970154, 0.47008514404296875]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 402 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6956359148025513, 0.6936659216880798, 0.6851656436920166, 0.47308436036109924, 0.4717217981815338]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 403 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  2\n",
      "MMR:  0.5\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6963589191436768, 0.47609743475914, 0.4757387936115265, 0.4743906855583191, 0.4720536470413208]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 404 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6955043077468872, 0.67798912525177, 0.4739311635494232, 0.47091948986053467, 0.4691527783870697]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 405 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7030290365219116, 0.6725769639015198, 0.4799315333366394, 0.4723014831542969, 0.46772998571395874]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 406 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6991610527038574, 0.6813549399375916, 0.4785177707672119, 0.47127941250801086, 0.46674832701683044]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 408 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.696709394454956, 0.696709394454956, 0.47131237387657166, 0.4700886011123657, 0.4700060784816742]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 409 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.696709394454956, 0.696709394454956, 0.47131237387657166, 0.4700886011123657, 0.4700060784816742]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 409 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7006677389144897, 0.6950864791870117, 0.4715910255908966, 0.4672068953514099, 0.46206048130989075]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 410 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  2\n",
      "MMR:  0.5\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6887983083724976, 0.46923744678497314, 0.4683520793914795, 0.46494388580322266, 0.46485844254493713]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 415 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6969388723373413, 0.6723743677139282, 0.47179126739501953, 0.4672556221485138, 0.46722978353500366]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 422 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7082086801528931, 0.6947938799858093, 0.4716276526451111, 0.4671947658061981, 0.46709293127059937]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 429 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6991900205612183, 0.6690530776977539, 0.4707157015800476, 0.46646201610565186, 0.4613833725452423]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 451 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7067874073982239, 0.6921423077583313, 0.4706409275531769, 0.46605125069618225, 0.46586713194847107]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 499 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6949477195739746, 0.6805546283721924, 0.47008073329925537, 0.4658249616622925, 0.465747594833374]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 500 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6925361752510071, 0.691106915473938, 0.6856001615524292, 0.6757563352584839, 0.466269850730896]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 502 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  2\n",
      "MMR:  0.5\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.711252748966217, 0.682510495185852, 0.6783539652824402, 0.6727062463760376, 0.4661426842212677]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 503 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6971005797386169, 0.6942101716995239, 0.6833296418190002, 0.4742963910102844, 0.4669750928878784]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 504 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.698598325252533, 0.6672196388244629, 0.4696093201637268, 0.46516740322113037, 0.46035945415496826]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 521 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7020320892333984, 0.4724707007408142, 0.4680652320384979, 0.46770545840263367, 0.46299731731414795]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 599 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6965976357460022, 0.4716414511203766, 0.4673125743865967, 0.46699056029319763, 0.46231958270072937]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 600 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7013487815856934, 0.4730340838432312, 0.468649297952652, 0.468098521232605, 0.46378690004348755]\n",
      "Question:  سرویسو فراخوانی کردم، روی HTTP کد 999 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7029299139976501, 0.6825307607650757, 0.47340673208236694, 0.4729401767253876, 0.47285139560699463]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10100 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7057802677154541, 0.47502297163009644, 0.47427570819854736, 0.47387075424194336, 0.4737488627433777]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10101گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7080622315406799, 0.47278276085853577, 0.4724603295326233, 0.472327321767807, 0.47203823924064636]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10200 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7038083076477051, 0.6816352009773254, 0.47402113676071167, 0.4737167954444885, 0.47356531023979187]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10201 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7099989056587219, 0.6846840977668762, 0.4754799008369446, 0.4751729369163513, 0.47501206398010254]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10202 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7060894966125488, 0.4755936861038208, 0.47478291392326355, 0.4744291305541992, 0.4742961823940277]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10203 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7103704810142517, 0.7004815340042114, 0.47628551721572876, 0.47553613781929016, 0.47515103220939636]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10204 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.707766592502594, 0.6826932430267334, 0.4747498333454132, 0.47440820932388306, 0.47425392270088196]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10205 گرفتم، این کد چیه؟ \n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7091753482818604, 0.6829380393028259, 0.4749660789966583, 0.4745686650276184, 0.47445157170295715]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10206 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7093119025230408, 0.6806018948554993, 0.4716342091560364, 0.4715529978275299, 0.4711899757385254]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10300 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7061465978622437, 0.7030854225158691, 0.47498294711112976, 0.47417399287223816, 0.4737027585506439]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10301 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7111119627952576, 0.6867983341217041, 0.6857001185417175, 0.6821312308311462, 0.4840553402900696]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10302گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7071845531463623, 0.6898510456085205, 0.6884809732437134, 0.6840574145317078, 0.4754643440246582]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10303 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7118127942085266, 0.6818602681159973, 0.47601714730262756, 0.47520712018013, 0.4746774435043335]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10305 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7074481248855591, 0.6947781443595886, 0.6864734292030334, 0.6848394274711609, 0.478069543838501]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10307 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7124904990196228, 0.6841663718223572, 0.6835024356842041, 0.6805820465087891, 0.4655657708644867]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10308 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  4\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.701972484588623, 0.6799110770225525, 0.47230732440948486, 0.4719173014163971, 0.4718376696109772]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10400 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.699721097946167, 0.691710352897644, 0.6888078451156616, 0.4788675010204315, 0.4768047034740448]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10401 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6985859274864197, 0.6879593729972839, 0.48574334383010864, 0.4757949709892273, 0.47375574707984924]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10403 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6985140442848206, 0.6818431615829468, 0.4800173044204712, 0.4761139154434204, 0.4740641415119171]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10404 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7061399221420288, 0.6833415627479553, 0.4820171594619751, 0.4771624505519867, 0.47506481409072876]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10405 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7085801362991333, 0.47738081216812134, 0.47556355595588684, 0.47473183274269104, 0.4743196368217468]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10406 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.706863522529602, 0.6920487284660339, 0.6832941770553589, 0.47866585850715637, 0.476433128118515]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10407 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7075532078742981, 0.48413148522377014, 0.4812071919441223, 0.4746495187282562, 0.47376346588134766]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10408 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7095096707344055, 0.47712603211402893, 0.4750117063522339, 0.47399553656578064, 0.47393688559532166]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10409 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6833725571632385, 0.4993993043899536, 0.47476208209991455, 0.47368770837783813, 0.47368124127388]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10410 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  2\n",
      "MMR:  0.5\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6970048546791077, 0.6807279586791992, 0.47505107522010803, 0.47297871112823486, 0.4727177023887634]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10411 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7053192257881165, 0.6825730204582214, 0.4790744483470917, 0.4781324565410614, 0.47533905506134033]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10412 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7010254859924316, 0.6808751821517944, 0.6806853413581848, 0.47656434774398804, 0.47437766194343567]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10413 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6964386105537415, 0.475547194480896, 0.47543835639953613, 0.4733186364173889, 0.47281619906425476]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10414 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6968042850494385, 0.6796291470527649, 0.6791460514068604, 0.4745635390281677, 0.47225233912467957]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10415 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6935563087463379, 0.6784030795097351, 0.6782463192939758, 0.47339051961898804, 0.4714079797267914]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10416 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7014206647872925, 0.6802062392234802, 0.680180013179779, 0.47617900371551514, 0.4740639328956604]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10417 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7005277872085571, 0.6801565289497375, 0.47228676080703735, 0.4719300866127014, 0.4718717038631439]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10500 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7023371458053589, 0.6846365332603455, 0.6805145740509033, 0.47132301330566406, 0.4709692895412445]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10501 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7032057046890259, 0.6963946223258972, 0.6835994124412537, 0.6808403134346008, 0.6789401173591614]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10502 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7029716968536377, 0.6927406191825867, 0.6864955425262451, 0.6811796426773071, 0.6770931482315063]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10503 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7021450400352478, 0.6831846833229065, 0.680449903011322, 0.4788146913051605, 0.47058990597724915]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10504 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7014742493629456, 0.6927332878112793, 0.6827397346496582, 0.6797544360160828, 0.6785458326339722]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 10505 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  5\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7077627182006836, 0.4723871052265167, 0.47234562039375305, 0.4717966318130493, 0.4709359407424927]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9102 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7045683264732361, 0.47214728593826294, 0.47209250926971436, 0.4717162847518921, 0.4707198739051819]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9103 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.5001635551452637, 0.472878098487854, 0.47283193469047546, 0.47236189246177673, 0.47142431139945984]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9104 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.5045828223228455, 0.472941130399704, 0.472891628742218, 0.4714883863925934, 0.4712124168872833]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9105 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7056261897087097, 0.47268080711364746, 0.472200483083725, 0.4712718427181244, 0.47093889117240906]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9106 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.5034911632537842, 0.47265446186065674, 0.4721825420856476, 0.47123512625694275, 0.4709599018096924]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9107 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7021805047988892, 0.47311586141586304, 0.4730689525604248, 0.4726440906524658, 0.47165897488594055]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9108 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.497965008020401, 0.4723042845726013, 0.47224825620651245, 0.471861332654953, 0.4708883762359619]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9109 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.49838608503341675, 0.4729542136192322, 0.47290876507759094, 0.47245222330093384, 0.471479207277298]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9110 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6950421929359436, 0.6840761303901672, 0.4769001007080078, 0.4764096438884735, 0.4754919707775116]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9111 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7027561664581299, 0.6894963979721069, 0.48040908575057983, 0.4798981547355652, 0.47887057065963745]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9112 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  3\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.6951951384544373, 0.47689297795295715, 0.4714733362197876, 0.46831125020980835, 0.4664890766143799]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9113 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7009104490280151, 0.6813254356384277, 0.47711795568466187, 0.47661688923835754, 0.4757119417190552]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9114 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  2\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7024983167648315, 0.47289031744003296, 0.47284528613090515, 0.4723327159881592, 0.47145920991897583]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9150 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.700249433517456, 0.4713309705257416, 0.4712681472301483, 0.47096720337867737, 0.4696868360042572]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9151 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n",
      "Similarity scores: [0.7013217210769653, 0.4733782112598419, 0.4733273983001709, 0.47293996810913086, 0.47195208072662354]\n",
      "Question:  سرویسو فراخوانی کردم، کد پاسخ 9190 گرفتم، این کد چیه؟\n",
      "Found:  1\n",
      "Rank:  1\n",
      "MMR:  1.0\n",
      "Retrieved Contexts count:  1\n",
      "======================================\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "def generate_retrieval_system_report(vector_db_client, questions, expected_contexts, collection_name, top_k):\n",
    "    reports = {\n",
    "        'question': [],\n",
    "        'expected_context': [],\n",
    "        'retrieved_contexts': [],\n",
    "        'rank': [],\n",
    "        'recall@k': [],\n",
    "        'MMR': [],\n",
    "    }\n",
    "    for question, expected_context in zip(questions,expected_contexts):\n",
    "        retrieved_contexts = retrieve_relevant_documents(vector_db_client, question, collection_name, embedding_model, top_k)\n",
    "        metrics = evaluate_retrieval(retrieved_contexts, expected_context, top_k=top_k)\n",
    "        rank = metrics['rank']\n",
    "        recall_at_k = metrics['recall@k']\n",
    "        mmr = metrics['MMR']\n",
    "        reports['question'].append(question)\n",
    "        reports['expected_context'].append(expected_context)\n",
    "        reports['retrieved_contexts'].append(retrieved_contexts)\n",
    "        reports['rank'].append(rank)\n",
    "        reports['recall@k'].append(recall_at_k)\n",
    "        reports['MMR'].append(mmr)\n",
    "        print(\"Question: \",question)\n",
    "        print(\"Found: \",recall_at_k)\n",
    "        print(\"Rank: \", rank)\n",
    "        print(\"MMR: \", mmr)\n",
    "        print(\"Retrieved Contexts count: \", len(retrieved_contexts))\n",
    "        print(\"======================================\")\n",
    "        print(\"======================================\")\n",
    "    return reports\n",
    "\n",
    "retrieval_system_report = generate_retrieval_system_report(vector_db_client, questions, contexts, collection_name, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_retrieval_system_report(reports):\n",
    "    df_reports = pd.DataFrame(reports)\n",
    "    df_reports.to_csv('retrieval_system_report.csv', index=False)\n",
    "    # print(\"Cosine Similarity Average:\",df_reports['cosine'].mean(), '%')\n",
    "    # print(\"Sum processing time:\",df_reports['processing_time'].astype(float).sum(), 'sec')\n",
    "    # print(\"Report saved to the file successfully.\")\n",
    "    return df_reports\n",
    "\n",
    "df_retrieval_system_report = save_retrieval_system_report(retrieval_system_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_context</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>rank</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>MMR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x trace id برای چیه؟</td>\n",
       "      <td>در راستاي افزایش دقت لاگ تراکنش ها و تسهیل فرآ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x-trace-id کجاها استفاده میشه؟</td>\n",
       "      <td>در راستاي افزایش دقت لاگ تراکنش ها و تسهیل فرآ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>سرویسای دیگه هم   به افرا  اضافه میشه یا فقط ه...</td>\n",
       "      <td>در صورت ارائه هر سرویس جدید قابل استفاده توسط ...</td>\n",
       "      <td>[افرا به عنوان اپراتور خدمات انتظامی کشور سروی...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>چقدر وقت دارم صورتحسابو پرداخت کنم؟</td>\n",
       "      <td>فاکتورهای موضوع قرارداد با مشتری، با رعایت الز...</td>\n",
       "      <td>[مطابق قرارداد مشتری موظف است کلیه مغایرت های ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>چجوری میتونم برای مشتریم دسترسی بگیرم؟</td>\n",
       "      <td>در صورت استفاده سرویس ها توسط مشتریان زیرمجموع...</td>\n",
       "      <td>[اطلاعات دسترسی پنل یعنی نام کاربری (user name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>سرویسو فراخوانی کردم، کد وضعیت 202 گرفتم، این ...</td>\n",
       "      <td>کد وضعیت ۲۰۲ از سمت سرویس دهنده دریافت میشود و...</td>\n",
       "      <td>[کد وضعیت 200 از سمت سرویس دهنده دریافت میشود ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>سرویسو فراخوانی کردم، کد وضعیت 401 گرفتم، این ...</td>\n",
       "      <td>این کد وضعیت از سمت سرویس دهنده دریافت میشود و...</td>\n",
       "      <td>[کد وضعیت 404 ارسالی توسط سرویس دهنده، به معنا...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0                                x trace id برای چیه؟   \n",
       "1                      x-trace-id کجاها استفاده میشه؟   \n",
       "14  سرویسای دیگه هم   به افرا  اضافه میشه یا فقط ه...   \n",
       "24                چقدر وقت دارم صورتحسابو پرداخت کنم؟   \n",
       "28             چجوری میتونم برای مشتریم دسترسی بگیرم؟   \n",
       "37  سرویسو فراخوانی کردم، کد وضعیت 202 گرفتم، این ...   \n",
       "39  سرویسو فراخوانی کردم، کد وضعیت 401 گرفتم، این ...   \n",
       "\n",
       "                                     expected_context  \\\n",
       "0   در راستاي افزایش دقت لاگ تراکنش ها و تسهیل فرآ...   \n",
       "1   در راستاي افزایش دقت لاگ تراکنش ها و تسهیل فرآ...   \n",
       "14  در صورت ارائه هر سرویس جدید قابل استفاده توسط ...   \n",
       "24  فاکتورهای موضوع قرارداد با مشتری، با رعایت الز...   \n",
       "28  در صورت استفاده سرویس ها توسط مشتریان زیرمجموع...   \n",
       "37  کد وضعیت ۲۰۲ از سمت سرویس دهنده دریافت میشود و...   \n",
       "39  این کد وضعیت از سمت سرویس دهنده دریافت میشود و...   \n",
       "\n",
       "                                   retrieved_contexts  rank  recall@k  MMR  \n",
       "0                                                  []   NaN         0  0.0  \n",
       "1                                                  []   NaN         0  0.0  \n",
       "14  [افرا به عنوان اپراتور خدمات انتظامی کشور سروی...   NaN         0  0.0  \n",
       "24  [مطابق قرارداد مشتری موظف است کلیه مغایرت های ...   NaN         0  0.0  \n",
       "28  [اطلاعات دسترسی پنل یعنی نام کاربری (user name...   NaN         0  0.0  \n",
       "37  [کد وضعیت 200 از سمت سرویس دهنده دریافت میشود ...   NaN         0  0.0  \n",
       "39  [کد وضعیت 404 ارسالی توسط سرویس دهنده، به معنا...   NaN         0  0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_retrieval_system_report[df_retrieval_system_report['recall@k'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_context</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>rank</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>MMR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>اگه درباره پنل مشتریان سوال داشتم، چیکار کنم؟</td>\n",
       "      <td>اطلاعات دسترسی پنل یعنی نام کاربری (user name)...</td>\n",
       "      <td>[پنل مشتریان پلتفرم افرا به آدرس https://apiad...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>جزئیات بیشتر ورودی و خروجی سرویس‌ها را چجوری ت...</td>\n",
       "      <td>پس از عقد قرارداد و درخواست سرویس ها توسط مشتر...</td>\n",
       "      <td>[در صورت استفاده سرویس ها توسط مشتریان زیرمجمو...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>در صورت وجود مغایرت در صورت وضعیت ماهانه چه اق...</td>\n",
       "      <td>در صورت بروز هرگونه اختلاف و مغایرت بین طرفین ...</td>\n",
       "      <td>[مطابق قرارداد مشتری موظف است کلیه مغایرت های ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>سرویسو فراخوانی کردم، روی HTTP کد 403 گرفتم، ا...</td>\n",
       "      <td>کد 403 در HTTP به معنای دسترسی غیرمجاز است - ا...</td>\n",
       "      <td>[کد 503 در HTTP به معنای این است که سرویس در د...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>سرویسو فراخوانی کردم، روی HTTP کد 410 گرفتم، ا...</td>\n",
       "      <td>کد 410 در HTTP به معنای این است که منبع درخواس...</td>\n",
       "      <td>[کد وضعیت 410 ارسالی توسط سرویس دهنده، به این ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>سرویسای دیگه هم   به افرا  اضافه میشه یا فقط ه...</td>\n",
       "      <td>در صورت ارائه هر سرویس جدید قابل استفاده توسط ...</td>\n",
       "      <td>[افرا به عنوان اپراتور خدمات انتظامی کشور سروی...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>چقدر وقت دارم صورتحسابو پرداخت کنم؟</td>\n",
       "      <td>فاکتورهای موضوع قرارداد با مشتری، با رعایت الز...</td>\n",
       "      <td>[مطابق قرارداد مشتری موظف است کلیه مغایرت های ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>چجوری میتونم برای مشتریم دسترسی بگیرم؟</td>\n",
       "      <td>در صورت استفاده سرویس ها توسط مشتریان زیرمجموع...</td>\n",
       "      <td>[اطلاعات دسترسی پنل یعنی نام کاربری (user name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>سرویسو فراخوانی کردم، کد وضعیت 202 گرفتم، این ...</td>\n",
       "      <td>کد وضعیت ۲۰۲ از سمت سرویس دهنده دریافت میشود و...</td>\n",
       "      <td>[کد وضعیت 200 از سمت سرویس دهنده دریافت میشود ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>سرویسو فراخوانی کردم، کد وضعیت 401 گرفتم، این ...</td>\n",
       "      <td>این کد وضعیت از سمت سرویس دهنده دریافت میشود و...</td>\n",
       "      <td>[کد وضعیت 404 ارسالی توسط سرویس دهنده، به معنا...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "10      اگه درباره پنل مشتریان سوال داشتم، چیکار کنم؟   \n",
       "34  جزئیات بیشتر ورودی و خروجی سرویس‌ها را چجوری ت...   \n",
       "26  در صورت وجود مغایرت در صورت وضعیت ماهانه چه اق...   \n",
       "58  سرویسو فراخوانی کردم، روی HTTP کد 403 گرفتم، ا...   \n",
       "65  سرویسو فراخوانی کردم، روی HTTP کد 410 گرفتم، ا...   \n",
       "..                                                ...   \n",
       "14  سرویسای دیگه هم   به افرا  اضافه میشه یا فقط ه...   \n",
       "24                چقدر وقت دارم صورتحسابو پرداخت کنم؟   \n",
       "28             چجوری میتونم برای مشتریم دسترسی بگیرم؟   \n",
       "37  سرویسو فراخوانی کردم، کد وضعیت 202 گرفتم، این ...   \n",
       "39  سرویسو فراخوانی کردم، کد وضعیت 401 گرفتم، این ...   \n",
       "\n",
       "                                     expected_context  \\\n",
       "10  اطلاعات دسترسی پنل یعنی نام کاربری (user name)...   \n",
       "34  پس از عقد قرارداد و درخواست سرویس ها توسط مشتر...   \n",
       "26  در صورت بروز هرگونه اختلاف و مغایرت بین طرفین ...   \n",
       "58  کد 403 در HTTP به معنای دسترسی غیرمجاز است - ا...   \n",
       "65  کد 410 در HTTP به معنای این است که منبع درخواس...   \n",
       "..                                                ...   \n",
       "14  در صورت ارائه هر سرویس جدید قابل استفاده توسط ...   \n",
       "24  فاکتورهای موضوع قرارداد با مشتری، با رعایت الز...   \n",
       "28  در صورت استفاده سرویس ها توسط مشتریان زیرمجموع...   \n",
       "37  کد وضعیت ۲۰۲ از سمت سرویس دهنده دریافت میشود و...   \n",
       "39  این کد وضعیت از سمت سرویس دهنده دریافت میشود و...   \n",
       "\n",
       "                                   retrieved_contexts  rank  recall@k  MMR  \n",
       "10  [پنل مشتریان پلتفرم افرا به آدرس https://apiad...   2.0         1  0.5  \n",
       "34  [در صورت استفاده سرویس ها توسط مشتریان زیرمجمو...   2.0         1  0.5  \n",
       "26  [مطابق قرارداد مشتری موظف است کلیه مغایرت های ...   2.0         1  0.5  \n",
       "58  [کد 503 در HTTP به معنای این است که سرویس در د...   2.0         1  0.5  \n",
       "65  [کد وضعیت 410 ارسالی توسط سرویس دهنده، به این ...   2.0         1  0.5  \n",
       "..                                                ...   ...       ...  ...  \n",
       "14  [افرا به عنوان اپراتور خدمات انتظامی کشور سروی...   NaN         0  0.0  \n",
       "24  [مطابق قرارداد مشتری موظف است کلیه مغایرت های ...   NaN         0  0.0  \n",
       "28  [اطلاعات دسترسی پنل یعنی نام کاربری (user name...   NaN         0  0.0  \n",
       "37  [کد وضعیت 200 از سمت سرویس دهنده دریافت میشود ...   NaN         0  0.0  \n",
       "39  [کد وضعیت 404 ارسالی توسط سرویس دهنده، به معنا...   NaN         0  0.0  \n",
       "\n",
       "[134 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_retrieval_system_report.sort_values(by = ['rank'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Load LLM for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (20000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "context_window_size = 20000\n",
    "\n",
    "def load_llm_model(model_path=\"./models/gemma-3-4b-it.Q2_K/gemma-3-4b-it.Q2_K.Q8_0.gguf\",chat_format='gemma'):\n",
    "    \"\"\"Load and configure the LLM for text generation\"\"\"\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        chat_format=chat_format,\n",
    "        n_gpu_layers=-1,  # Use all available GPU layers\n",
    "        n_ctx=context_window_size,       # Context window size\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "\n",
    "# Define LLM models details like: path and chat_format\n",
    "llm_models_details = {\n",
    "    'dorna-llama3-8b-q8' : {'path': './models/Dorna-Llama3-8B-Instruct-GGUF-Q8/dorna-llama3-8b-instruct.Q8_0.gguf',\n",
    "                            'chat_format': 'llama-3'},\n",
    "    'deepseek-r1-7b-qwen' : {'path': './models/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B.Q8_0.gguf',\n",
    "                            'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-q2': {'path':'./models/gemma-3-4b-it.Q2_K/gemma-3-4b-it.Q2_K.gguf',\n",
    "                     'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-q8': {'path':'./models/gemma-3-4b-it.Q8_0/gemma-3-4b-it.Q8_0.gguf',\n",
    "                      'chat_format': 'gemma'},\n",
    "    'gemma-3-4b-fp16': {'path':'./models/gemma-3-4b-it.fp16/gemma-3-4b-it.fp16.gguf',\n",
    "                        'chat_format': 'gemma'},\n",
    "    'gemma-3-12b-q4': {'path':'./models/gemma-3-12b-it.Q4_0/gemma-3-12b-it-q4_0.gguf',\n",
    "                        'chat_format': 'gemma'}\n",
    "    }\n",
    "\n",
    "# Load Llama model\n",
    "target_llm_model = 'gemma-3-12b-q4'\n",
    "llm_model_path, llm_chat_format = llm_models_details[target_llm_model]['path'], llm_models_details[target_llm_model]['chat_format']\n",
    "llm = load_llm_model(llm_model_path, llm_chat_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "USER_PROMPT_TEMPLATE = '''\n",
    "<role>You are “Technical Assistant.”</role>\n",
    "\n",
    "<goal>\n",
    "Answer technical questions concisely, using **only** chat history or retrieved context, and reply in formal Persian.\n",
    "</goal>\n",
    "\n",
    "<precedence>\n",
    "1.<guardrails> 2.<behavior> 3.Latest user request 4.History 5.Context\n",
    "</precedence>\n",
    "\n",
    "<behavior>\n",
    "  • Pay attention to conversation history always.  \n",
    "  • Follow-ups like «بیشتر توضیح بده / مثال بزن» → extend last answer and explain more details.  \n",
    "  • If no clear keyword remains, ask one clarifying question (Persian).  \n",
    "  • If info is absent in both history & context → output «اطلاعاتی در دسترس نیست. با ادمین تماس بگیرید.»  \n",
    "</behavior>\n",
    "\n",
    "<steps>\n",
    "  0. **Short-Reply**: new question → 2 brief sentence.  \n",
    "  1. **Detailed-Reply**: user asks for more → up to 10 sentences, only existing info.  \n",
    "  2. **Clarify**: ambiguous or keywordless → ask 1 clarifying Q.  \n",
    "  3. **No-Info**: nothing relevant → fixed no-info sentence.\n",
    "</steps>\n",
    "\n",
    "<style>\n",
    "Formal Persian; no emojis; bullets only on request.\n",
    "</style>\n",
    "\n",
    "<guardrails>\n",
    "  • No external knowledge.  \n",
    "  • Reveal technical details only when asked.  \n",
    "  • Reject out-of-scope requests with «درخواست خارج از حیطه است».  \n",
    "  • All answers Persian.  \n",
    "  • If origin queried → «من توسط شرکت امیرمهدی ساخته شده‌ام.» (never mention Google).\n",
    "</guardrails>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Conversation history:\n",
    "{history}\n",
    "\n",
    "User:\n",
    "{prompt}\n",
    "'''\n",
    "\n",
    "\n",
    "SUMMARY_PROMPT_TEMPLATE = \"\"\"\n",
    "<role>You are “Conversation Summarizer.”</role>\n",
    "\n",
    "<goal>\n",
    "Return a **concise Persian chronology** of every user–assistant turn in the form:  \n",
    "«اول، کاربر … و دستیار … . دوم، کاربر … و دستیار … . سوم، … .»  \n",
    "Continue sequentially until the last turn; each clause must capture only the key request and the core response.\n",
    "</goal>\n",
    "\n",
    "<precedence>\n",
    "1. <guardrails>  \n",
    "2. <behavior>  \n",
    "3. Conversation history  \n",
    "</precedence>\n",
    "\n",
    "<behavior>\n",
    "  <rule>Process the entire history (summarise internally if > 2 000 characters).</rule>\n",
    "  <rule>Extract just the essential intent of each user message and the essence of the assistant’s reply.</rule>\n",
    "  <rule>Limit the whole summary to ≤ 60 Persian words **or** 6 numbered pairs—whichever comes first.</rule>\n",
    "  <rule>Use the fixed numbered format; no extra commentary, emojis, or citations.</rule>\n",
    "</behavior>\n",
    "\n",
    "<steps>\n",
    "  <step0 title=\"Chronology\">\n",
    "    <trigger>Always (history is present)</trigger>\n",
    "    <action>Return the numbered Persian chronology.</action>\n",
    "  </step0>\n",
    "</steps>\n",
    "\n",
    "<style>\n",
    "  <item>Formal Persian; no bullet points, no emojis.</item>\n",
    "</style>\n",
    "\n",
    "<guardrails>\n",
    "  <rule>No external knowledge or personal assumptions.</rule>\n",
    "  <rule>All outputs must be in Persian.</rule>\n",
    "</guardrails>\n",
    "\n",
    "Conversation History:\n",
    "{conversation_history}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EXTRACT_KEYWORD_PROMPT_TEMPLATE = \"\"\"\n",
    "<role>You are “Keyword Extractor.”</role>\n",
    "\n",
    "<goal>\n",
    "Return the **maximum 10 most relevant Persian keywords / phrases** from the user prompt.  \n",
    "Example – input:  \n",
    "«سلام … کد پاسخ ۵۰۳ معنیش چیه؟ ممنون» → output: «کد پاسخ ۵۰۳»\n",
    "</goal>\n",
    "\n",
    "<precedence>\n",
    "1. <guardrails>  \n",
    "2. <behavior>  \n",
    "3. Latest user prompt\n",
    "</precedence>\n",
    "\n",
    "<behavior>\n",
    "  <rule>Remove greetings, courtesy phrases, punctuation, and stop-words.</rule>\n",
    "  <rule>Otherwise return up to ten Persian words that best capture the technical intent (e.g., «اتصال پایگاه‌داده»).</rule>\n",
    "  <rule>Output must be one Persian phrase, no extra characters, no brackets, no emojis.</rule>\n",
    "  <rule>If no meaningful technical term exists, output the full text without extraction.</rule>\n",
    "</behavior>\n",
    "\n",
    "<steps>\n",
    "  <step0 title=\"Extract\">\n",
    "    <trigger>Always</trigger>\n",
    "    <action>Return the cleaned Persian keywords/phrases per rules.</action>\n",
    "  </step0>\n",
    "</steps>\n",
    "\n",
    "<style>\n",
    "  <item>Output exactly the keywords phrase—nothing else.</item>\n",
    "</style>\n",
    "\n",
    "<guardrails>\n",
    "  <rule>No external knowledge; extract only from the prompt text.</rule>\n",
    "  <rule>All outputs must be in Persian.</rule>\n",
    "</guardrails>\n",
    "\n",
    "User Prompt:\n",
    "{user_prompt}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "def summarize_history_with_llm(history,llm=llm):\n",
    "    \"\"\"Summarize query using the LLM\"\"\"\n",
    "    summary_prompt = SUMMARY_PROMPT_TEMPLATE.format(conversation_history = history)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": summary_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.0  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    # .split('</think>')[-1] if the model thinks!\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    print(f\"Summarized version: {response_clean}\")\n",
    "\n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def extract_keyword_with_llm(user_prompt,llm=llm):\n",
    "    \"\"\"Extract Keyword using the LLM\"\"\"\n",
    "    extract_keyword_prompt = EXTRACT_KEYWORD_PROMPT_TEMPLATE.format(user_prompt = user_prompt)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": extract_keyword_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.0  # Low temperature for more deterministic responses\n",
    "    )\n",
    "    # .split('</think>')[-1] if the model thinks!\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    print(f\"Extracted version: {response_clean}\")\n",
    "\n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def retrieve_context(vector_db_client, query, collection_name=collection_name, embedding_model=embedding_model,top_k=top_k):\n",
    "    \"\"\"Retrieve relevant context based on the query\"\"\"\n",
    "    docs = retrieve_relevant_documents(vector_db_client, query, collection_name, embedding_model,top_k)\n",
    "    for doc in docs:\n",
    "        print(f\"Retrieved document: {doc[:100]}...\")\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "\n",
    "def generate_response_stream(model_input, llm=llm):\n",
    "    \"\"\"Generate streaming response using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{model_input}\"}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1,  # Low temperature for more deterministic responses\n",
    "        repeat_penalty= 1.2,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in response:\n",
    "        delta = chunk['choices'][0]['delta']\n",
    "        if 'content' in delta:\n",
    "            content = delta['content']\n",
    "            print(content, end='', flush=True)\n",
    "            full_response += content\n",
    "            yield content\n",
    "\n",
    "\n",
    "def generate_response(model_input, llm=llm):\n",
    "    \"\"\"Generate response using the LLM\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": model_input}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1,  # Low temperature for more deterministic responses\n",
    "      repeat_penalty= 1.2\n",
    "    )\n",
    "    # .split('</think>')[-1] if the model thinks!\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    print(response_clean)\n",
    "\n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def rag_chat(user_query, collection_name=collection_name, history=None, stream=False):\n",
    "    \"\"\"Complete RAG pipeline: Retrieve → Generate → Respond\"\"\"\n",
    "    if history is None:\n",
    "        history = conversation_history\n",
    "    \n",
    "    summarized_history_text = \"\"\n",
    "\n",
    "    # Format conversation history\n",
    "    history_text = \"\\n\".join(history)\n",
    "\n",
    "    if history:\n",
    "        summarized_history_text = summarize_history_with_llm(history_text)\n",
    "        \n",
    "    \n",
    "    user_query_keywords = extract_keyword_with_llm(user_query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(vector_db_client, user_query_keywords, collection_name, embedding_model,top_k)\n",
    "\n",
    "        \n",
    "    \n",
    "    # Create prompt with context and history\n",
    "    prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        history=summarized_history_text,\n",
    "        context=context, \n",
    "        prompt=user_query\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        # Generate streaming response\n",
    "        response = \"\"\n",
    "        response_stream = generate_response_stream(prompt)\n",
    "        for chunk in response_stream:\n",
    "            response += chunk\n",
    "    else:\n",
    "        # Generate response (non-stream)\n",
    "        response = generate_response(prompt)\n",
    "\n",
    "\n",
    "    history.append(f\"User: {user_query}\")\n",
    "    history.append(f\"Assistant: {response}\")\n",
    "\n",
    "\n",
    "    \n",
    "    llm.reset()\n",
    "    return response,context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10. Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: x-trace-id کجاها استفاده میشه؟\n",
      "Extracted version: x-trace-id\n",
      "Similarity scores: [0.683688759803772, 0.6830585598945618, 0.6352226138114929, 0.6352226138114929, 0.445304811000824]\n",
      "Retrieved document: در راستاي افزایش دقت لاگ تراکنش ها و تسهیل فرآیندهای RA، مولفه x-trace-id یا X trace id كه در هدر (H...\n",
      "Retrieved document: در راستاي افزایش دقت لاگ تراکنش ها و تسهیل فرآیندهای RA، مولفه X-trace-id كه در هدر (Header) پاسخ بر...\n",
      "Retrieved document: جهت حل مشکل تمدید بسته ها و استفاده از سرویس بسته های متنوع، متغیری به نام باندل آی دی یا bundle -id...\n",
      "Retrieved document: جهت حل مشکل تمدید بسته ها و استفاده از سرویس بسته های متنوع، متغیری به نام باندل آی دی یا bundle -id...\n",
      "x-trace-id در هدر پاسخ برگشتی ارسال می‌شود و باید توسط دریافت‌کننده سرویس ذخیر"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Time the response\u001b[39;00m\n\u001b[32m      9\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response, context = \u001b[43mrag_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m end = time.time()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcessing time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 351\u001b[39m, in \u001b[36mrag_chat\u001b[39m\u001b[34m(user_query, collection_name, history, stream)\u001b[39m\n\u001b[32m    349\u001b[39m     response = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    350\u001b[39m     response_stream = generate_response_stream(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    354\u001b[39m     \u001b[38;5;66;03m# Generate response (non-stream)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 291\u001b[39m, in \u001b[36mgenerate_response_stream\u001b[39m\u001b[34m(model_input, llm)\u001b[39m\n\u001b[32m    282\u001b[39m response = llm.create_chat_completion(\n\u001b[32m    283\u001b[39m     messages=messages,\n\u001b[32m    284\u001b[39m     top_p=\u001b[32m0.85\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    287\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    288\u001b[39m )\n\u001b[32m    290\u001b[39m full_response = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchoices\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdelta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/chatbot/venv/lib/python3.12/site-packages/llama_cpp/llama_chat_format.py:314\u001b[39m, in \u001b[36m_convert_text_completion_chunks_to_chat\u001b[39m\u001b[34m(chunks)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_text_completion_chunks_to_chat\u001b[39m(\n\u001b[32m    312\u001b[39m     chunks: Iterator[llama_types.CreateCompletionStreamResponse],\n\u001b[32m    313\u001b[39m ) -> Iterator[llama_types.ChatCompletionChunk]:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/chatbot/venv/lib/python3.12/site-packages/llama_cpp/llama.py:1320\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1318\u001b[39m finish_reason = \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1319\u001b[39m multibyte_fix = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/chatbot/venv/lib/python3.12/site-packages/llama_cpp/llama.py:914\u001b[39m, in \u001b[36mLlama.generate\u001b[39m\u001b[34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[39m\n\u001b[32m    912\u001b[39m \u001b[38;5;28mself\u001b[39m.eval(tokens)\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx < \u001b[38;5;28mself\u001b[39m.n_tokens:\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     token = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalize_nl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalize_nl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     sample_idx += \u001b[32m1\u001b[39m\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[32m    935\u001b[39m         \u001b[38;5;28mself\u001b[39m._input_ids[: sample_idx], \u001b[38;5;28mself\u001b[39m._scores[sample_idx - \u001b[38;5;28mself\u001b[39m.n_tokens, :]\n\u001b[32m    936\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/chatbot/venv/lib/python3.12/site-packages/llama_cpp/llama.py:815\u001b[39m, in \u001b[36mLlama.sample\u001b[39m\u001b[34m(self, top_k, top_p, min_p, typical_p, temp, repeat_penalty, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_eta, mirostat_tau, penalize_nl, logits_processor, grammar, idx)\u001b[39m\n\u001b[32m    812\u001b[39m ridx = idx - \u001b[38;5;28mself\u001b[39m.n_tokens \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m token = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mridx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tmp_sampler:\n\u001b[32m    817\u001b[39m     \u001b[38;5;28mself\u001b[39m._sampler = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/chatbot/venv/lib/python3.12/site-packages/llama_cpp/_internals.py:866\u001b[39m, in \u001b[36mLlamaSampler.sample\u001b[39m\u001b[34m(self, ctx, idx)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m ctx.ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_sampler_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example 1: Basic question (Without Summary)\n",
    "query1 = questions[1]\n",
    "print(f\"User query: {query1}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query1,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: میشه بیشتر راجع به این توضیح بدی؟\n",
      "Summarized version: اول، کاربر پرسید x-trace-id کجاها استفاده میشه؟ و دستیار پاسخ داد در هدر پاسخ برگشتی ارسال می‌شود و برای افزایش دقت لاگ تراکنش‌ها صادر می‌شود.\n",
      "Extracted version: توضیح\n",
      "Similarity scores: [0.6261348128318787, 0.19380313158035278, 0.19331420958042145, 0.193064883351326, 0.1930556744337082]\n",
      "Retrieved document: در رنج کدهای پاسخ 10xxx، درخواست کاربر به درستی به تامین کننده نهایی (سازمان مقصد) ارسال شده است. دق...\n",
      "بله، می‌توانم بیشتر توضیح دهم. x-trace-id در هدر پاسخ برگشتی ارسال می‌شود تا به افزایش دقت لاگ تراکنش‌ها کمک کند و امکان ردیابی دقیق‌تر جریان درخواست را فراهم آورد. این شناسه برای شناسایی یک تراکنش خاص در سیستم‌های مختلف مورد استفاده قرار می‌گیرد.---\n",
      "Processing time: 5.36 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Follow-up question\n",
    "query2 = \"میشه بیشتر راجع به این توضیح بدی؟\"\n",
    "print(f\"User query: {query2}\")\n",
    "\n",
    "# Time the response (using existing conversation history)\n",
    "start = time.time()\n",
    "response, context = rag_chat(query2,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Follow-up question\n",
    "query3 = \"تو رو کی ساخته؟\"\n",
    "print(f\"User query: {query3}\")\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query3,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Follow-up question\n",
    "query4 = \"اولش ازت چی پرسیدم؟\"\n",
    "print(f\"User query: {query4}\")\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query4,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Follow-up question\n",
    "query5 = \"بعدش ازت چی پرسیدم؟\"\n",
    "print(f\"User query: {query5}\")\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query5,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Different topic question\n",
    "query6 = questions[29]\n",
    "print(f\"User query: {query6}\")\n",
    "\n",
    "# Reset conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query6,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"---\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Follow-up question\n",
    "query7 = \"چرا؟\"\n",
    "print(f\"User query: {query7}\")\n",
    "\n",
    "# Reset conversation history\n",
    "# conversation_history = []\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response, context = rag_chat(query7,stream = True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 11. RAG System Evaluation\n",
    "\n",
    "Test with more complex queries to evaluate retrieval performance and answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_TEMPLATE = \"\"\"\n",
    "<role>You are an Answer-Quality Judge.</role>\n",
    "\n",
    "<goal>\n",
    "  Return ONE and only ONE JSON object with exactly two keys:\n",
    "    \"score\" → an integer 1-10 (no quotes, no decimals)\n",
    "    \"tags\"  → a non-empty JSON array whose elements come only from:\n",
    "              helpful, partially_helpful, unhelpful, off_topic,\n",
    "              unclear, incorrect, incomplete, redundant,\n",
    "              verbose\n",
    "</goal>\n",
    "\n",
    "<rules>\n",
    "  <rule>No other keys, comments, or text are allowed.</rule>\n",
    "  <rule>Do NOT echo the question or answers.</rule>\n",
    "  <rule>Do NOT wrap the JSON in markdown.</rule>\n",
    "  <rule>If you violate any rule, output exactly\n",
    "        {{\\\"error\\\":\\\"invalid output\\\"}} and STOP.</rule>\n",
    "</rules>\n",
    "\n",
    "<output_format>\n",
    "<![CDATA[\n",
    "{{\"score\": <1-10>, \"tags\": [\"<allowed_tag>\", …]}}\n",
    "]]>\n",
    "</output_format>\n",
    "\n",
    "Give me score and tags in JSON format.\n",
    "\n",
    "Question: {question}\n",
    "Answer (Model): {generated_response}\n",
    "Answer (Ground truth): {ground_truth_answer}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_response_llm_as_a_judge(question, generated_response, ground_truth_answer):\n",
    "    \n",
    "    \"\"\"Evaluation for generated response by model vs ground truth answer\"\"\"\n",
    "    judge_prompt = JUDGE_TEMPLATE.format(question = question, generated_response = generated_response, ground_truth_answer = ground_truth_answer)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": judge_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        top_p=0.85,\n",
    "        temperature=0.1,  # Low temperature for more deterministic responses\n",
    "        repeat_penalty= 1.2 \n",
    "        )\n",
    "    response_clean = response['choices'][0]['message']['content']\n",
    "    \n",
    "    print(\"LLM as a judge result:\", response_clean)\n",
    "    \n",
    "    return response_clean\n",
    "\n",
    "\n",
    "def evaluate_generated_response_cosine(generated_response, ground_truth_answer,embedding_model=embedding_model):\n",
    "    \n",
    "    \"\"\"Evaluation for generated response by model vs ground truth answer\"\"\"\n",
    "    generated_response_embeddings = embedding_model.encode_documents([generated_response])['dense'][0]\n",
    "    ground_truth_answer_embeddings = embedding_model.encode_documents([ground_truth_answer])['dense'][0]\n",
    "\n",
    "    cosine_score_raw = util.pytorch_cos_sim(generated_response_embeddings, ground_truth_answer_embeddings)\n",
    "    \n",
    "    cosine_score = round(float(cosine_score_raw[0][0]) * 100, 2)\n",
    "    print(\"Cosine Similarity between generated response and ground truth answer:\", cosine_score)\n",
    "    \n",
    "    return cosine_score\n",
    "\n",
    "\n",
    "def evaluate_generated_response_prf(generated_response, ground_truth_answer):\n",
    "    \n",
    "    \"\"\"Evaluation for generated response by model vs ground truth answer\"\"\"\n",
    "\n",
    "    P_raw, R_raw, F1_raw = score([generated_response], [ground_truth_answer], lang='en') # model_type='distilbert-base-uncased'\n",
    "    P = round(float(P_raw[0]) * 100, 2)\n",
    "    R = round(float(R_raw[0]) * 100, 2)\n",
    "    F1 = round(float(F1_raw[0]) * 100, 2)\n",
    "    print(\"Precision: \", P)\n",
    "    print(\"Recall: \", R)\n",
    "    print(\"F1 Score: \", F1)\n",
    "    \n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat_with_processing_time(query):\n",
    "    \"\"\"RAG chat + processing time \"\"\"    \n",
    "    print(f\"User query: {query}\")\n",
    "\n",
    "    # Time the response\n",
    "    start = time.time()\n",
    "    response, context = rag_chat(query,stream=False)\n",
    "    end = time.time()\n",
    "    \n",
    "    processing_time = f\"{end - start:.2f}\"\n",
    "    print(f\"\\nProcessing time: {processing_time} seconds\")\n",
    "    return response, context, processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judgment_json_from_llm_response(llm_response):\n",
    "    judgment_json_str = re.sub(r'^```json\\s*|\\s*```$', '', llm_response).strip()\n",
    "\n",
    "    judgment_json = json.loads(judgment_json_str)\n",
    "    return judgment_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_system_report(questions=questions,answers=answers):\n",
    "    reports = {\n",
    "        'question': [],\n",
    "        'response': [],\n",
    "        'answer': [],\n",
    "        'judgement score' : [],\n",
    "        'judgement_tags':[],\n",
    "        'cosine': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'context': [],\n",
    "        'processing_time': []\n",
    "    }\n",
    "    for question,answer in zip(questions,answers):\n",
    "        response, context, processing_time = rag_chat_with_processing_time(question)\n",
    "        llm_judgement = evaluate_generated_response_llm_as_a_judge(question,response,answer)\n",
    "        llm_judgement_json = extract_judgment_json_from_llm_response(llm_judgement)\n",
    "        llm_judgement_judgment_score = llm_judgement_json['score']\n",
    "        llm_judgement_judgment_tags = ', '.join(llm_judgement_json['tags'])\n",
    "        cosine = evaluate_generated_response_cosine(response, answer,embedding_model=embedding_model)\n",
    "        precision, recall, f1_score = evaluate_generated_response_prf(response, answer)\n",
    "        reports['question'].append(question)\n",
    "        reports['response'].append(response)\n",
    "        reports['answer'].append(answer)\n",
    "        reports['judgement score'].append(llm_judgement_judgment_score)\n",
    "        reports['judgement_tags'].append(llm_judgement_judgment_tags)\n",
    "        reports['cosine'].append(cosine)\n",
    "        reports['precision'].append(precision)\n",
    "        reports['recall'].append(recall)\n",
    "        reports['f1_score'].append(f1_score)\n",
    "        reports['context'].append(context)\n",
    "        reports['processing_time'].append(processing_time)\n",
    "        print(\"======================================\")\n",
    "        print(\"======================================\")\n",
    "    return reports\n",
    "\n",
    "reports = generate_rag_system_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rag_system_report(reports):\n",
    "    df_reports = pd.DataFrame(reports)\n",
    "    df_reports.to_csv('rag_system_report.csv', index=False)\n",
    "    print(\"Cosine Similarity Average:\",df_reports['cosine'].mean(), '%')\n",
    "    print(\"Judgement Score Average:\",df_reports['judgement score'].mean(), '%')\n",
    "    print(\"Sum processing time:\",df_reports['processing_time'].astype(float).sum(), 'sec')\n",
    "    print(\"Report saved to the file successfully.\")\n",
    "    return df_reports\n",
    "\n",
    "df_rag_system_report = save_rag_system_report(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rag_system_report['processing_time'].astype(float).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
